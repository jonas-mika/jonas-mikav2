---
title: Maximum Likelihood Estimation
description:
course: Applied Statistics
tags: []
published: 2023-04-17
lastEdited: 2023-04-17
---

If we encounter a sample in the real world, we might want to model it with a
theoretical distribution. To do this, we need to _estimate_ the parameters of
the theoretical distribution. Usually, we can find a natural sample analogue: If
we, for example, want to fit a normal distribution we can compute estimates for
$\mu$ and $\sigma^2$ through the sample mean and sample variance.

But, how did we arrive at the formulas for the mean and sample variance in the
first place? How would we estimate the parameters of a uniform or exponential
distribution? What we need is a generic approach to finding estimators of
distribution parameters $\theta$. **Maximum likelihood estimation** (MLE) is one
way to construct the 'best-fitting' estimators for a parameter with a specific
definition of 'best-fitting' in mind.

With MLE in our toolkit, we can find theoretical estimators (that we can use to
compute empirical estimates) of any distribution parameter of any distribution.
Furthermore, the maximum likelihood estimates have some properties that make
them nice to work with.

## Maximum Likelihood Principle

---

Although the math behind maximum likelihood estimation might be daunting at
first, the general idea of the method is quite intuitive. Let's consider a
common example of MLE in a simple setting that we will use to build up the
general math along the way. Consider that you have collected the following ten
data points of flipping a coin, where a $1$ denotes `Head` and a $0$ denotes
`Tail`. In this example, we will consider `Head` a success.

$$
\text{Sample} = \{1, 1, 0, 1, 1, 1, 0, 1, 1, 0\}
$$

You are now tasked with finding the 'best-fitting' distribution parameters given
this sample. Each data point is clearly a `Bernoulli` trial. Let's denote a
single trial as a random process modelled through a discrete random variable
$X : \text{Getting Heads}$. As $X \sim Ber$, we need to estimate the parameter
$p$, the probability of success, to construct the probability mass function
(PMF) $f_X$.

$$
f_X(x)=P(X=x)=
\begin{cases}
p     &, \text{if } x = 1\\
(1-p) &, \text{if } x - 0\\
0     &, \text{else}
\end{cases}
$$

> At this point you might be saying: "But this is trivial! Just count the number
> of successes and divide it by the total number of data points in the
> sample." - Indeed, this is exactly what we will be doing. Intuitively you make
> the same assumptions about parameter estimation as MLE. Our goal now is to
> mathematically motivate your intuition.

How do we go about finding an estimate for $p$? We might start thinking about
the **parameter space**, so the range of all possible values $p$ can obtain.
Here, $p$ is a probability measure and therefore $0 \le p \le 1$. Clearly,
$p = 0$ cannot be true, since we have observed successes in our sample.
Similarly, $p = 1$ would be a very bad estimate, since we have also observed
failures in our sample. However, everything in the range $]0, 1[$ is technically
possible - with _some being values being more likely than others_.

For example, observing seven successes in ten trials with a success probability
of $p=0.01$ is very unlikely. It is for example more likely that $p=0.5$. This
kind of reasoning about the parameter space, in which we are trying to find an
estimate in, is exactly what underlies maximum likelihood estimation.

As the name suggests, maximum likelihood estimation tries to estimate the
parameters of a distribution such that **the observed sample is maximally
likely**. What does this mean specifically? We wish to find $p$ in a way that
when we create a huge number of samples we get more samples that resemble our
observed sample, than with any other $p$.

And, just like that, we have understood the big idea for MLE: Approximate the
parameters of a distribution such that the sample you have observed is maximally
likely. How do we quantify this? As always, with a function. But not just any
function: MLE constructs the so-called **likelihood function** $L(\theta)$,
which models the probability of observing a sample under some assumed
distribution. For a data set of some random variable $X$, which is a i.i.d
sequence of realisations of the form $X_1, X_2, ..., X_n$, we write it as

$$
\begin{align*}
L(\theta) &= P(X_1 = x_1 \cap X_2 = x_2 \cap \ldots \cap X_n = x_n) \\
          &= P(X_1 = x_1) \cdot P(X_2 = x_2) \cdot \ldots \cdot P(X_n = x_n) \\
          &= f_X(x_1) \cdot f_X(x_2) \cdot \ldots \cdot f_X(x_n) \\
          &= \prod_i^n f_X(x_i)
\end{align*}
$$

_Note, $f_X$ is the probability mass function (PMF) if $X$ is discrete, and the
probability distribution function (PDF) if $X$ is continuous._

<br />

Now, since $f_X$ is parametrised with the function parameters, the likelihood
function $L$ itself is parametrised and we wish to find a single or set of
parameters $\theta$, for which $L$ is largest. Mathematically, this is the
maximum of the function $L$ and we can find it by setting the gradient vector
equal to the zero vector (when estimating multiple parameters) or the derivative
equal to zero and then solve the equation.

In real-world settings, instead of differentiating the long product in the
likelihood function (which is annoying because of the product rule), we often
find the **log-likelihood function**, which is simply the log of the likelihood
function, and turns the long product into a long sum ($log(ab)=log(a)+log(b)$).

$$
\begin{align*}
l(\theta) &= log(L(\theta)) \\
          &= log(\prod_i^n f_X(x_i)) \\
          &= \sum_i^n log(f_X(x_i)) \\
\end{align*}
$$

> I want to emphasize, that this step is a pure _mathematical convenience_ that
> makes our lives simpler. It works because the log transformation of any
> function $f$ does not change the critical points.

<br />

Let's return to our example and compute $p$, the probability of success, from
the sample using maximum likelihood estimation. First, we construct the
likelihood function. In this case, the parameter of interest is $p$. We
construct $L(p)$ as the product of the probabilities of all data points in the
sample, as given by the PMF of $X \sim Ber$.

$$
\begin{align*}
L(\theta) &= P(X_1 = 1 \cap X_2 = 1 \cap \ldots \cap X_n = 0) \\
          &= P(X_1 = 1) \cdot P(X_2 = 1) \cdot \ldots \cdot P(X_n = 0) \\
          &= f_X(1) \cdot f_X(1) \cdot \ldots \cdot f_X(0) \\
          &= p \cdot p \cdot \ldots \cdot (1-p) \\
          &= p^7 \cdot (1-p)^3 \\
\end{align*}
$$

For ease of computation, we compute the log-likelihood to have an easier time
differentiating.

$$
\begin{align*}
l(p) &= log(L(p)) \\
          &= log(p^7 \cdot (1-p)^3 ) \\
          &= 7 \cdot log(p) + 3 \cdot log(1-p)
\end{align*}
$$

Now, we can differentiate $l(p)$:

$$
\frac{\delta l}{\delta p} 7\ log(p) + 3\ log(1-p) = \frac{7}{p} - \frac{3}{1-p}
$$

And then solve for zero to obtain $p$, for which $l$ and therefore $L$ is
maximal.

$$
\begin{align*}
\frac{\delta l}{\delta p} &= 0 \\
\frac{7}{p} - \frac{3}{1-p} &= 0 \\
7(1-p) - 3p &= 0 \\
7 - 7p - 3p &= 0 \\
-10p &= -7 \\
p &= \frac{-7}{-10} = \frac{7}{10} = 0.7 \\
\end{align*}
$$

We have found our success probability to be $p=0.7$, which is exactly what we
assumed all the way along. As it was constructed according to the maximum
likelihood principle, it is the parameter that makes the sample that we have
observed **most likely** (which makes sense, since we have observed 7 successes
in 10 trials). Again, it is important to notice, that it is an estimate and not
universal truth. Having 7 successes in 10 trials could have easily occurred with
a fair coin ($p=0.5$), it is just less likely than with $p=0.7$.

> Instead of a specific sample we can compute MLE estimates on general samples.
> In the above scenario, for any sample from a Bernoulli distribution with $k$
> successes in $n$ trials the MLE estimate is simply the ratio of success to
> total trials, $\hat{p} = \frac{k}{n}$.

## Properties of MLE

---

Apart from the fact that the maximum likelihood principle provides a general
principle to construct estimators, one can also show that ML estimators have
some nice asymptotic properties, which make it a well-suited tool in many
applications:

1. **Invariance of MLE**

   $\Theta_{ML}$ is _invariant_ to transformations of the parameter, i.e. if
   $\Theta_{ML}$ is MLE for $\theta$, then $g(\Theta_{ML})=g(\theta)$.

   The above property is called the **invariance principle**. It means that if
   we have found a maximum likelihood estimator $\Theta_{ML}$for some model
   parameter $\theta$, then the MLE for any function $g(\theta)$ is
   $g(\Theta_{ML})$. This can i.e. be seen from the MLE for the variance of the
   normal distribution. Taking the square root of this MLE, yields the MLE for
   the standard deviation.

2. **Unbiased Estimator**

   $\Theta_{ML}$ is _asymptotically unbiased_, i.e.
   $lim_{n \to
   \infty}E[\Theta_{ML}]=\theta$.

   Although it can be shown that some MLE are slightly biased (i.e. the MLE for
   the variance of a normal distribution), we observe that the MLE are
   asymptotically unbiased, meaning that for an infinitely large sample, the
   bias is $0$.

3. **Central Limit Theorem**

   $\Theta_{ML}$ can be approximated as a normal random variable, and can be
   transformed into a $N(0,1)$ distribution through the CLT.
