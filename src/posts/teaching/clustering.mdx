---
title: Clustering
description:
course: Machine Learning
tags: []
published: 11-22-2022
lastEdited: 11-22-2022
---

This week is going to be our last regular exercise class together, so before jumping into this last piece of content of the course, I would like to
thank you for the great attendance in all my classes. I hope that my classes have been a valuable additional learning resource for you next to the
lectures and reading materials. In the next phase of the course, you will be working on a group project - you are of course still more than welcome to
come to the exercise classes during the regular hours to get help in case you are stuck or just get overall feedback on how your project is going.

But now to clustering, the very last topic that we are going to cover. Continuing last week’s content of dimensionality reduction, clustering is also
in the domain of unsupervised learning.

Formally, **cluster analysis** or **clustering** is defined as the task of grouping a set of objects (samples) into groups (called **clusters**) in a
sense that objects that are more similar (in some sense) to each other than to those in other groups. Therefore, we can think of clustering as the
problem of identifying natural occurring groups in data, although we have no knowledge about these groups.

Cluster analysis is a very common tool of exploratory data analysis and general statistical data analysis, as we humans generally like to find
patterns like groups in seemingly unstructured data.

We are going to discover two methods, **k-means clustering** and **hierarchical clustering** that define similarity of objects in slightly different
ways, which leads to different groupings.

## K-Means Clustering

---

K-Means clustering is one of the most well-known and simple ways of clustering data into precisely $k$ partitions. In being partition-based, every
sample sample $x_i$ is in one and only one data group $C_i$.

K-means clustering tries to optimise the **total within-cluster deviance** This metric is the sum of all the **within-cluster deviances,** therefore

$$
\text{Total Within-Class Deviance}=W_{C_1}+...+W_{C_k}
$$

The within-class deviance simply measures the spread of the data samples within a class. In Euclidean space we can assign a centre $r_k$ to each
cluster $C_i$ , which we can then use to measure the within cluster deviance as the sum over all squared distances of the in-class samples to the
centre. The more widespread the cluster, the larger the within-class deviance.

$$
W_{C_k}(r_k)=\sum_{i:x_i\in C_k}||x_i-r_k||^2
$$

Now, we wish to find a partition of the data samples $x_1, ..., x_n$ into the $k$ partitions $C_1, ..,C_k$ and doing so by minimising the total
within-class deviance. Before constructing an optimisation function and numerical solver, take a second to ponder about the following two observations
about within-class deviances:

1. If we know all cluster centres $r_1, ... , r_k$, then we would assign each sample to the closest centre
2. If we know which observation belongs to a cluster $C_i$, then we would choose the cluster centre $r_i$ to be the average of all samples in the
   cluster $C_i$.

These two thought experiments give rise to an iterative optimisation algorithm known as **k-means clustering**.

1. Initialise the $k$ cluster centres $r_1,...,r_k$ (usually by choosing $k$ number of datapoints from the data at random)
2. Assign all data points to the closest cluster centre.
3. Recompute the cluster centres as the mean of the assigned cluster in $(2)$.

This algorithm is run until convergence. Logically, the algorithm will always stop in a finite number of iterations, as either the total
within-cluster deviance is unchanged by the reallocation of points - in which case we converged and stop the algorithm - or it decreases. This means
that we never revisit the same partitioning of data, and there are only a finite number of partitions to go through.

## Hierarchical Clustering Methods

---

**Hierarchical clustering** (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of
clusters. Strategies for hierarchical clustering generally fall into two types:

- **Agglomerative**: This is a bottom-up approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the
  hierarchy. → At each step given $K$ clusters, merge two most similar to get $K-1$ clusters.
- **Divisive**: This is a top-down approach: all observations start in one cluster, and splits are performed recursively as one moves down the
  hierarchy. → At each step given $K$clusters, divide one cluster to get $K+1$ clusters. For a single divisive step, we need to find the cluster of
  greatest dissimilarity among all clusters and for this cluster need to find the division into two clusters that creates most dissimilarity. Thus, we
  need to decide both \*which **\***cluster to divide and _how_ to divide it. For a cluster of size $N$ there are $\frac{2^N}{2}-1=2^{N-1}-1$ possible
  ways of splitting the cluster. Thus, the algorithm is infeasible on larger data and we use approximations and heuristics to cut down runtime.

**Quantifying Dissimilarity of Clusters**

With either strategy, hierarchical clustering is based on a measure of **dissimilarity** between clusters. In agglomerative clustering, we seek to
merge the two _most similar clusters_ together at each merge, while in divisive clustering we seek to divide a cluster, such that the two resulting
clusters are maximally dissimilar.

The dissimilarity between two clusters of data points can be based (in many ways) on distance between two data points $d(x,y)$, which can be combined
in different strategies of quantifying cluster distance $D(C_i, C_j)$:

Single-Link Clustering: $D(C_i, C_j)=min\ \{d(x,y)\ |\ x\in C_i, y\in C_j\}$

Complete-Link Clustering: $D(C_i, C_j)=max\ \{d(x,y)\ |\ x\in C_i, y\in C_j\}$

Group-Average Clustering: $D(C_i, C_j)=avg\ \{d(x,y)\ |\ x\in C_i, y\in C_j\}$
