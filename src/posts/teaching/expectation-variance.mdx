---
title: Expectation and Variance
description:
course: Applied Statistics
tags: []
published: 03-06-2023
lastEdited: 03-06-2023
---

Random variables are complicated objects that contain all probabilistic
information of the random experiments that they model. However, sometimes we may
wish to look at the randomness in a process in a simpler way, e.g. through a
single number instead of a function. In order to do so, we need to find and
compute metrics with meaningful interpretations.

## Expectation

---

The **expected value** (_also: mean or average_) $E[X]$ (also: $EX=E(X)=\mu_X$)
is the most intuitive and likewise most important metric, when dealing with
random variables.

As the name suggests, it gives us an intuition, which realisation/ outcome of
the random process modelled through a random variable we should expect. In an
empirical setting, we can observe the theoretical quantity of the expectation of
a random variable by taking the sample mean for a (infinitely) large amount of
trials of the experiment. The LNN (law of large numbers) tells us that the
sample mean will converge towards the theoretical expectation.

### Expectation of Discrete Random Variables

---

The expectation/ expected value of a discrete random variable $X$ taking the
values $x_1, x_2, ...$ and with the probability mass function $p$ is the number
computed through the weighted average as follows:

$$
E[X]=\sum_{x \in R_X}xP(X=x)=\sum_{x \in R_X}xp_X(x)
$$

The expected value $E[X]$ is called the expected value or mean of $X.$ The
graphical intuition is to think of the expectation/ mean as the center of
gravity, when viewing the probability mass function as a scale.

### Expectation of Continuous Random Variables

---

The expectation of a continuous random variable $X$ with probability density
function $f$ is the analogous computation to the discrete case and is computed
as follows:

$$
E[X]=\int_{-\infty}^{\infty}xf_X(x)\ dx
$$

## Variance

---

In probability theory and statistics, variance is the expectation of the squared
deviation of a random variable from its mean. In other words, it measures how
spread out the distribution of a random variable is.

The variance is the square of the standard deviation, the second central moment
of a distribution, and the covariance of the random variable with itself, and it
is often represented by $\sigma^2$, $sÂ²$ or $Var(X)$.

### Definition

---

The **variance** $Var[X]$, $Var(X)$, $\sigma^2$ or $s^2$ of a random variable
$X$ is the real-valued constant that is always greater than $0$, since the
average of the squared deviation from the mean is always positive.

$$
Var(X) = E[(X-E[X])^2]
$$

Large values mean that $(X-E[X])^2$ is often large, so $X$ often takes values
far from its mean. This means that the distribution is very spread out. On the
other hand, a low variance means that the distribution is concentrated around
its average.

_Note, that if we did not square the difference between $X$ and its expected
value $E[X]$, the result would always be 0, since on average, we expect the
variable to be 0 away from the mean._

### Alternative Computation

---

In order to compute the variance of a random variable, we need to compute the
expected value of our random variable $X$ transformed with $g(x)=(x-E[X])^2$,
where $E[X]$ is a constant. To compute this, we need to use the
change-of-variable formula from that we have already discovered. However, there
exists a simpler way of computing variances. With this rule we make two steps:
First we compute the expected value $E[X]$, then we compute the so-called second
moment of $X$, which is the expected value of the random variable squared,
$E[X^2]$.

$$
\begin{align*}
  Var[X] & = E[(X-E[X])^2]\\
  & = E[X^2 - 2XE[X]+E[X]^2] \\
  & = E[X^2] - 2E[X]E[X] + E[X]^2 \\
  &=E[X^2]-E[X]^2
\end{align*}
$$

The given equation is usually easier to work with than $Var[X]=E[(X-E[X])^2]$,
since we only need to find $E[X^2]$ using the change-of-variable formula and
then subtract the squared expected value, which we already computed.

_Notice, that this formula easily be used if we want to compute the expected
value of the transformed variable $E[X^2]$, if we know the variance and
expectation of $X$ in advance:_

$$
E[X^2]=E[X]^2+Var[X]
$$

## Standard Deviation

---

The _variance_ of some distribution gives us a feel for how spread out the
values that the random variable $X$ can obtain are in reality. High variances
imply a large spread, while low variances imply a rather concentrated
distribution around the expected value.

However, the variance measures the expected squared deviation from the mean, and
is thus measured in different units. If our random variable $X$ measures human
heights in $cm$, then the $Var[X]$ is in $cm^2$. To account for this issue, we
can define another measure, called the _standard deviation._ Mathematically, the
standard deviation is easy enough defined. We simply take the square root of the
computed variance.

$$
SD[X]=\sigma_X=\sqrt{Var[X]}
$$

## Computation with Random Variables

---

It is not rare, that random variables are transformed through simple arithmetic
operations. Consider e.g. instead of being interested in the random variable $X$
modelling the sum of two independent rolls with a die, we could be looking at
the random variable $Y=2X+1$, which for each the values that can be obtained are
determined by the sum of the two die rolls and are transformed in a way, that
they are multiplied by 2 and added 1 to. Instead of reformulating the entire
range $Y$ is defined on and computing the probability distribution through the
PMF and CDF, we would like to be able to compute the expectation of this new
random variable fast and conveniently.

Luckily, the single-point summaries of distributions, the expectation and
variance of random variables have nice properties when it comes to
transformations.

### Change-of-Variable-Formula (also: _LOTUS_)

---

For any discrete random variable $X$ and some defined function $g:\R \to \R$
(i.e. $g(x)=2x+1$ from above), we can obtain the expected value of $X$ through
the following formula:

$$
E[g(X)]=\sum_{x_k \in R_X}g(x_k)P(X=x_k)
$$

And for the continuous case, we similarly observe:

$$
E[g(X)]=\int_{-\infty}^{\infty}g(x)f_X(x)\ dx
$$

The change-of-variable formula is the most generic way of recomputing the
expectation for a transformed random variable, since it works with any kind of
transformation function $g$. This is sensible, since all it does is applying the
function to the former range of the random variable and incorporating this new
range into the regular formulas for computing expectations.

### Linearity of Expectation

---

However, for linear transformations of the type $g(x)=ax+b$, we can use the
LOTUS-formula to show that:

$$
E[aX+b]=a\cdot E[X]+b
$$

This allows us to recompute the expectation of a linearly transformed random
variable simply by transforming the computed value of the expectation itself.

_Note, that since $E[aX]=E[X_1]+E[X_2] + ... + E[X_a]$, the total expectation of
$a$independent trials of the same random phenomena modelled through $X$ is
simply the sum each single expectation. Adding a constant to some random
variable $E[X+b]$ simply shifts the range $R_X$ by the constant but leaves the
probability distribution untouched. Thus, the expectation also simply shifts by
that constant $E[X]+b$_

$$
\begin{align*}
  E[aX+b]
  &=\sum_{x \in R_X} (ax+b)p_X(x) \\
  &=\sum_{x \in R_X} axp_X(x)+bp_X(x)\\
  &= \sum_{x \in R_X} axp_X(x)+ \sum_{x \in R_X} bp_X(x)\\
  &= a\sum_{x \in R_X} xp_X(x)+ b\sum_{x \in R_X} p_X(x)\\
  &=a E[X]+b
\end{align*}
$$

### Non-Linearity of Variance

---

Now, that we have discovered the _linearity of expectation_, the question
arises, of whether the variance if also linear dependent on linear
transformations of some random variable. It turns out, that for the same generic
linear transformation $g(x)=ax+b$, the variance is affected in the following
way.

$$
Var[aX+b]=a^2Var[X]
$$

_Note, that the variance is insensitive to the vertical shift over b and is
scaled by the power of the scaling factor $a$ in front of $X$. This means that
the variance doesn't grow linearly (as did the expectation)_

However, there is one important case, in which the variance behaves like a
linear operation. That is when we take the sum of independent random variables.

$$
Var[X]=Var[X_1]+Var[X_2]+...+Var[X_n]
$$
