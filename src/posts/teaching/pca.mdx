---
title: Dimensionality Reduction with PCA
description:
course: Machine Learning
tags: []
published: 11-18-2022
lastEdited: 11-18-2022
---

This week we are for the first time in this course leaving the supervised problem space of machine learning (meaning that our algorithms are
supervised by some ground truth of how they are expected to behave), to tackle _unsupervised_ problem settings.

In machine learning, two of the main problems in the unsupervised learning space are **dimensionality reduction** and _clustering_. Today we are going
to explore the unsupervised dimensionality reduction technique called **principle component analysis (PCA)**.

## Dimensionality Reduction

---

**Dimensionality reduction** is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional
representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension.

There are several motivations for being interested in dimensionality reduction:

1. **Computational Performance**

   When ML models learn and predict, they do computations on samples in the number of features we consider. If we first compress the number of
   features through dimensionality reduction, we save (possibly) valuable computations, which means we can train and deploy our models faster.

2. **Model Performance**

   Some models suffer from the _curse of dimensionality_, especially if the number of samples is relatively low compared to the number of features.
   Such models actually see performance gains if they are trained on compressed features.

3. **Visualisation**

   We cannot visualise datasets in higher dimensions (more than two features + target in the supervised setting). Dimensionality reduction gives us a
   way of displaying high-dimensional data in a lower dimension, which unlocks all kinds of visual exploratory analysis tools (like histograms,
   empirical PMF, KDE, scatter plots, etc.), which help us understand the underlying data better.

## Principle Component Analysis (PCA)

---

Principle component analysis (PCA) is an unsupervised learning technique for dimensionality reduction. This means, that given some feature matrix
$X\in\R^{n\times p}$ (which is a collection of samples $x_i$ from a random vector $X\in \R^p$) with potentially large $p$, we wish to find a
compression of these features into a smaller subset of features - ergo: reduce the dimensionality of our problem.

PCA seeks to transform the $p$ dimensional feature space into a $1\le k\le p-1$ output space. Before considering $k$ principle component, let’s simply
the problem and think about the case when $k=1$, so when we wish to construct a single feature out of the $p$ features we have available.
Mathematically, we are considering an initial random vector $X\in\R^p$, consisting of $p$ random variables

$$
X=(X_1, ...,X_p)
$$

and want to construct a new feature $Z_1$ by doing some computation on the random vector

$$
Z_1=\text{PCA}(X)
$$

Before trying to understand how PCA is approaching this question, think about this problem yourself? If you have knowledge about each of the
individual features $X_1,...,X_p$ through their empirical distribution, how would you combine them into a single feature $Z_1$ and still keep up
performance when training on training on this single feature?

Well, PCA has a very specific answer to this question. Firstly, it adds a convenience. It just determines that we are only to consider linear
transformations of the original features, so the construction of the feature is always going to be on the type:

$$
\begin{align*}Z_1&=a_{11}X_1+...+a_{1p}X_p\\&=\sum_ia_{1i}X_i\end{align*}
$$

Realise, that this is a _huge_ restriction. We could have defined interaction terms between features, squares, cubes and other crazy things. But: PCA
assumes linearity, so all we are left to do is find a set of $p$ scalars $a_{11}, ..., a_{1p}$ that scale each feature respectively, which we sum over
to construct $Z_1$. Before moving on to the more interesting part, let’s take a second to realise that if both collect all scalars (we call them
**loading)** into a vector $a_1=\begin{bmatrix}a_{11}\dots a_{1p}\end{bmatrix}$ and multiply it with the feature random vector $X$, then we can
vectorise the linear computation

$$
Z_1=a_1^TX
$$

Now, the relevant question is how we choose this loading vector $a_1$. For all loadings $a_{1i}=1$, we are just summing over the random vectors to
construct a new feature, but is that really the best we can do? PCA does not think so: It says, that we would like to construct $Z_1$ in a way that we
persist maximal variance. This is sensible, because the higher the variance in our newly constructed feature, the higher the chance for it to do well
on our classes (As a counterexample, imagine that the new feature has variance $0$, this means you have a dataset of equal values, so nothing to learn
at all).

And, suddenly, we have an optimisation problem to determine the loading vector $a_1$, which is

$$
\begin{align*}
   maximise\ Var(Z_1)=a_1^T\Sigma a_1,
   \text{s.t. }\  a_1^Ta_1=1
\end{align*}
$$

Let’s unpack this. We said that we want to maximise the variance in the new feature, so $Var(Z_1)$. Now, we have defined $Z_1$ to be the linear
combination $a_1^TX$, so we are maximising $Var(a_1^TX)$. Now, reactivate your statistics knowledge, and remember that for a the variance of a random
vector is the covariance matrix $\Sigma$, and that for a scaled random variable $aX$, the variance is $Var(aX)=a^2Var(X)$. Since $a_1$ is a vector the
whole expression becomes $a_1^T\Sigma a_1$. Lastly, we constrain our loadings to have a dot product of $1$, which means we want $a_1$ to be a unit
vector. That is, because infinitely large values would artificially increase the variance, which is unintended. Now, we can combine the two expression
into a Lagrangian (optimisation under constraints) to obtain:

$$
\mathcal L(a_1)=a_1^T\Sigma a_1-\lambda_1(a_1^Ta_1-1)
$$

Multivariate calculus tells us how to solve this expression. We find the partial derivative with respect to each element of $a_1$ by deriving
$\mathcal L$ with respect to the entire $a_1$ vector. We obtain

$$
\frac{\delta \mathcal L}{\delta a_1}2\Sigma a_1 - 2\lambda _1a_1
$$

Now, setting this vector equation equal to the zero vector $0$, we obtain

$$
2\Sigma a_1 - 2\lambda _1a_1=0
$$

After dividing by two and adding $\lambda_1a_1$ on both sides we get

$$
\Sigma a_1=\lambda_1 a_1
$$

And which points you should realise that $a_1$ is an eigenvector to the eigenvalue $\lambda_1$ of the covariance matrix $\Sigma$ of the random vector
$X$ that we wanted to compress. Now, the only question is which eigenvalue/ eigenvector pair to choose - after all, there can be up to $p$ for a
$p\times p$ covariance matrix, right? Well, let’s have a another look at our initial optimisation, which was to maximise the variance in the new
feature $Var(a_1^TX)=a_1^T\Sigma a_1$ . Now, in the light of $a_1$ being an eigenvector to eigenvalue $\lambda_1$, we can write this expression by
replacing the matrix product with a scalar product, so

$$
a_1^T\lambda_1 a_1
$$

Now, we can pull out the scalar multiple $\lambda_1(a_1^Ta_1)$ and we suddenly realise that we constrained the dot product inside the parentheses to
be $1$ (because $a_1$ has to be a unit vector). This means that the final variance in the new feature $Z_1$ is going to be $Var(Z_1)=\lambda_1$, which
means that we naturally choose the eigenvector $a_1$ to the largest eigenvalue $\lambda_1$ as our vector of loadings.

Congratulations, you worked through all the math and saw, that if you want to compress any $p$ features into a single feature that has maximal
variance, then you need to find the eigenvector to the largest eigenvalue of the sample covariance matrix. This new feature will have a variance of
$\lambda_1$.

Unfortunately (or luckily), PCA doesn’t quite stop here, because there are many use-cases, where a single feature may not capture enough variance for
it to be useful. In these cases, we would like to create more features, which are called the second, third, etc. principle components. PCA tells us
that they should be constructed using the same idea to maximise variance, but _additionally should be perpendicular to all previous principle
components_. To account for this, we need to slightly change our optimising function:

$$
\begin{align*}
maximise\ Var(Z_1)=Var(a_2^TX)=a_2^T\Sigma a_2 \\
s.t.\ a_2^Ta_2=1, a_2^Ta_1=0
\end{align*}
$$

Which leads to the Lagrangian

$$
\mathcal L (a_2)=a_2^T\Sigma a_2-\lambda_2(a_2^Ta_2-1)-\mu(a_2^Ta_1)
$$

And continuing the math, we observe that, again, $a_2$ is an eigenvector to eigenvalue $\lambda_2$ to $\Sigma$, and we choose the second largest
eigenvalue to construct the second principle component. This pattern continuous on, so the matrix, whose column vectors are the eigenvectors of
$\Sigma$ (ordered in reverse order of their corresponding eigenvalue) is the matrix that creates the $p$ principle components (For the linear algebra
nerds: This matrix is also part of the eigendecomposition (diagonalisation of $\Sigma$ using a orthonormal basis of eigenvectors)).
