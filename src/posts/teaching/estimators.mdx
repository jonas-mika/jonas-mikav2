---
title: Estimators
description:
course: Applied Statistics
tags: []
published: 2023-04-16
lastEdited: 2023-04-16
---

So far we have calculated estimates for true distribution parameters or
attributes without thinking more about it. In this section, we seek to formalise
the idea by introducing the notation of estimators and biases in these
estimators.

## Estimators

---

Intuitively, we want to be able to characterise our data in some way. We want to
be able to characterise some unknown parameter $\Theta$, i.e. the centre of the
dataset, maybe its dispersion (variability) or model parameters for an
underlying theoretical distribution (i.e. the $min$ and $max$ for a uniform
distribution). To do so, we need some abstract concept - a formula - which
computes this estimate for us given the data at hand. We call the rule to
estimate some parameter an estimator and define it in the following way:

$$
T=h(X_1,  X_2, ..., X_n)
$$

The estimator $T$ is in itself a random variable, as it is a function taking in
a sequence of independent, identically distributed random variables (a dataset)
and computes a single value. The single value computed is called an estimate and
is formally denoted as $t$

$$
t=h(x_1, x_2, ..., x_n)
$$

**Example.** The most common estimator is the sample mean, which we already
encountered to estimate $\Theta$, the expectation, of the underlying
distribution of some dataset. Formally we define:

$$
T=\frac{X_1 + X_2 + ... + X_n}{n}
$$

From this estimator, we can compute an estimate $t$, the sample mean, of the
dataset (a realisation of some underlying theoretical distribution).

## Bias in Estimators

---

However, we could define any function to be an estimator for some parameter. How
do we decide if our chosen estimator is any good? To do this, we need some
methods to evaluate our estimator. One important one is the _bias_ of an
estimator. It tells us how far our estimator is on average from the real value
of $\Theta$.

It is a desirable property for an estimator to - on average - be equal to the
parameter it is supposed to estimate. We therefore formally define the property
of _bias_ of an estimator, which is defined as follows:

$$
E[T]=\Theta \Leftrightarrow E[|T-\Theta|]=0
$$

On average, our estimator is equal to the parameter $\Theta$ it is supposed to
estimate - or, in other words: The expected deviation is zero.

If $E[T]\ne\Theta$, we say that the estimator is biased. However, if we observe
constant bias, we might still be able to take this estimator into consideration
by accounting for the biased estimations.
