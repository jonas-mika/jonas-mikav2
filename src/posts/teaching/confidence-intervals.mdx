---
title: Confidence Intervals
description:
course: Applied Statistics
tags: []
published: 2023-04-24
lastEdited: 2023-04-24
---

So far, we have only dealt with finding the best possible _point estimator_. For
some random sample $x_1, x_2, ..., x_n$ from a sequence of independent,
identically distributed random variables $X_1, X_2, ..., X_n$, we defined an
estimator $T$ as a function of the sample, that outputs a single digit, that is
supposed to estimate a theoretical quantity of the distribution of $X$.

However, especially, when the size of the sample is small, the point estimate
produced by our estimator might be very off (since we are only confident that it
will converge towards the real-value for a large sample size). Thus, we cannot
be sure how close our estimated value is to the real value. The metric in itself
is therefore not robust, because it involves randomness in the samples collected
from the underlying population. Sampling the same amount again, might ie. give a
different mean than the initially computed. (_Single) point estimators_ are
therefore, after all, only estimates.

So far, we have only considered them to be _somewhat close_ to the _real
theoretical value_. In this section, we are trying to _quantify_ what we mean by
_somewhat close_ by introducing the concept of **Confidence Intervals (CI)** and
therefore looking at so-called _interval estimators._

## General Concept

---

For a random sample $x_1, x_2, \ldots, x_n$ from a sequence of i.i.d. random
variables $X_1, X_2, \ldots, X_n$ and some desired model parameter $\theta$, a
confidence interval is an interval $[\hat{\theta}_l,\hat{\theta}_h]$ that is
likely to includes the true value of $\theta$ with some probability $p$.

In interval eestimation, there are two important concepts:

1. **Length of the reported interval $\hat{\theta}_h - \hat{\theta}_l$**

   The length of the reported interval shows the precision with which we can
   estimate the model parameter $\theta$.

2. **Confidence Level**

   The confidence level shows how confident we are about the interval. The
   higher the confidence level, the more certain we are that the true value of
   $\theta$ is within the interval. Commonly, confidence levels are
   reported at a level of 95%, 99% of 99.5%.

_Note, that the two metrics are positively related to each other. That is, the
larger the reported interval, the higher the confidence level, but also the
less precise the interval. Thus, the goal is to find the smallest possible
interval with a critical confidence level._

## Prerequisites

---

In a second we are going to see _three_ plug-and-play formulas for _three_ cases
for constructing the confidence interval for the mean in three different cases.
It is _very easy_ to just use, but not understand, these formulas. My goal here
is to make you understand the theory in-depth, so that you would be able to
derive all formulas yourself if you wanted to. To be able to that, we will have
to revisit some simple examples from probability theory, which are going to be
crucial for understanding interval estimation.

<br />

**Probabilities on Intervals.** Let $X$ be a random variable with CDF
$F_X(x)=P(X \le x)$ and suppose that you are to find an interval $[x_l, x_h]$,
such that $100\cdot(1-a)\%$ of the probability mass of $X$ fall within that
interval. Formally written down, we are searching for

$$
P(x_l \le X \le x_u) = 1-a
$$

While there are infinitely many ways to choose such an interval, we are
considering the case of a **symmetric** interval. For the inner probability mass
to be $1-a$, the tail probabilities combined have to be $\alpha$. Under the
assumption that we construct a symmetric interval, each tail probability
has to be $\alpha / 2$. We denote these tail probabilities as:

$$
P(X \le x_l) = \alpha / 2, \text{and }
P(X \ge x_u) = \alpha / 2
$$

Following our definition of the CDF, we can equivalently write

$$
F_X(x_l) = \alpha / 2, \text{and }
F_X(x_u) = 1 - \alpha / 2
$$

_Note, that for the upper interval we use the complement probabitity to be able
to use the CDF, which by default computes left-tail probabilities. Here, $P(X
\ge x_u) = \alpha / 2$ implies $P(X \le x_u) = 1 - \alpha / 2$._

With this knowledge, we can simply use the inverse of the CDF $F_X^{-1}$ to
compute the interval boundaries $[x_l, x_u]$, as

$$
x_l = F_X^{-1}(\alpha / 2), \text{and }
x_u = F_X^{-1}(1 - \alpha / 2)
$$

The interval

$$
\left( F_X^{-1}(\alpha / 2),\ F_X^{-1}(1 - \alpha / 2) \right)
$$

, is called a $1-\alpha$ interval for $X$.

<br />

**Critical Values.** Before moving to confidence intervals, we have to get one
more terminology down. It is commonly used in literature when deriving the
intuition for confidence intervals, and so we have to know what it is. It will
make our formula for a $1-\alpha$ interval slightly nicer to read.

A **critical value** $z_p$ of a distribution $Z$ is defined as the value with
right-tail probability $p$

$$
P(Z \ge z_p) = p
$$

Equipped with this definition, we can re-write the interval boundaries. Because
the upper interval boundary $x_u$ has a right-tail probability of $\alpha / 2$
($P(X \ge x_u) = \alpha / 2$), we can call $x_u$, $z_{\alpha / 2}$ for $X$.
Similarly, because the lower interval boundary $x_l$ has a right-tail
probability of $1-\alpha / 2$, we can call $x_l$, $z_{1 - \alpha / 2}$ for $X$.

Therefore, we can re-write the entire $1-\alpha$ interval for $X$ as

$$
\left( z_{1-\alpha / 2}, z_{\alpha / 2}\right)
$$

In R, use can compute the critical value $z_p$ for any distribution $Z$, for
which you have the inverse CDF defined. Assuming, for example, a normal
distribution, we can compute $z_p$ as:

```r
qdist(p, lower.tail=F)
```

<br />

**Assuming Normality.** Now, if we assume normality of the process $X$, that is
$X\sim N(0,1)$, then the CDF and inverse CDF are well defined as $\Phi$ and
$\Phi^{-1}$ respectively. We can exchange the inverse CDF in our computation of
the interval boundaries.

$$
x_l = \Phi_X^{-1}(\alpha / 2), \text{and }
x_u = \Phi_X^{-1}(1 - \alpha / 2)
$$

However, the normal distribution is special because its probability density
function is symmetric, i.e. $f_X(-x) = f_X(x)$. For the CDF, this means that
$1-\Phi(x)=-\Phi(x)$ and for the inverse CDF, that $\Phi_X^{-1}(1 - x) =
-\Phi_X^{-1}(x)$. With $x=\alpha /2$ in the case of creating an interval, we get

$$
x_l = \Phi_X^{-1}(\alpha / 2), \text{and }
x_u = -\Phi_X^{-1}(\alpha / 2)
$$

Written as right-tail probabilities in the form of critical values we can say
that $z_{1 - \alpha / 2} = -z_{\alpha / 2}$, and so we only need to compute a
single critical value, $z_{\alpha / 2}$, to construct the $1-\alpha$ interval
for $Z$

$$
(-z_{\alpha / 2}, z_{\alpha / 2})
$$

## Confidence Intervals for the Mean

---

With this background knowledge, we can start to compute interval
estimators. In this course we will only construct confidence intervals for the
mean $\mu$ for a distribution $X$. We assume that we are given a realisation
$(x_1, \ldots, x_n)$ from the i.i.d. sequence of random variable $(X_1, \ldots,
X_n)$.

Let's recall that an unbiased ($E[\bar{X}_n]=\mu$) point estimator for the
parameter $\mu$ is the **sample mean** $\bar{X}_n$, which is defined as

$$
\bar{X}_n = \frac{X_1 + \ldots X_n}{n}
$$

We can compute the point estimate $\bar{x}_n$ by plugging a realised dataset
into the above formula. We will now develop a procedure to construct intervals
with different lengths and confidence levels to **quantify the uncertainty** of
our point estimate that arises from the sampling procedure.

### Case 1: CI for Mean of Normal Data with Known Variance

---

Under the assumption that our sample is **normally distributed** and that the
true **standard deviation** $\sigma$ is known, we can construct the
$100(1-\alpha)\%$ confidence interval relatively easy.

We realise that we wish to construct an interval around the true mean at
$1-\alpha$ confidence:

$$
P(L_n(X_1, \ldots, X_n) \le \mu \le U_n(X_1, \ldots, X_n)) = 1 - \alpha
$$

We will start out from our point estimator $\bar{X}_n$. By central limit
theorem, it will converge towards a normal distribution. In earlier weeks we
have already shown that $E[\bar{X}_n]=\mu$ and $Var[\bar{X}_n]=\sigma^2/n$ and
so $\bar{X}_n$ converges towards $N(\mu, \sigma / \sqrt{n})$. Properly
standardising this random distribution $(Z=\frac{X-\mu}{\sigma})$, we construct

$$
Z = \frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}}
$$

, wish follows a standard normal distribution. Now, we can write out the
following interval (just as in the **Prerequisites** section):

$$
P(c_l \le Z \le c_u) = 1 - \alpha
$$

Notice, the similarity to the definition of the confidence interval we are
after. It is not quite the same, but we are getting there. At this point, we
should remember all the work we have done about finding interval boundaries and
critical values. We know that we can construct a $1-\alpha$ interval for
a standard normal distribution through the interval $(z_{\alpha /2}, -z_{\alpha /
2}$. Since $Z$, our standardised sample mean, is a standard normal distribution,
we can replace $c_l$ with $-z_{\alpha / 2}$ and $c_u$ with $z_{\alpha / 2}$. Thus,

$$
P(-z_{\alpha / 2} \le Z \le z_{\alpha / 2}) = 1 - \alpha
$$

We will now do a series of transformations to this equation to transform it into
the confidence interval for the mean.

$$
\begin{align*}
1 - \alpha  &= P(-z_{\alpha / 2} \le Z \le z_{\alpha / 2}) \\
            &=P(-z_{\alpha / 2} \le \frac{\bar{X}_n-\mu}{\sigma / \sqrt{n}} \le z_{\alpha / 2}) \\
            &= P(-z_{\alpha / 2} \cdot \frac{\sigma}{\sqrt{n}} \le \bar{X}_n-\mu \le z_{\alpha / 2} \cdot \frac{\sigma}{\sqrt{n}}) \\
            &= P(-\bar{X}_n - z_{\alpha / 2} \cdot \frac{\sigma}{\sqrt{n}} \le -\mu \le -\bar{X}_n + z_{\alpha / 2} \cdot \frac{\sigma}{\sqrt{n}}) \\
            &= P(\bar{X}_n + z_{\alpha / 2 }\cdot \frac{\sigma}{\sqrt{n}} \ge \mu \ge \bar{X}_n - z_{\alpha / 2} \cdot \frac{\sigma}{\sqrt{n}}) \\
            &= P(\bar{X}_n - z_{\alpha / 2 }\cdot \frac{\sigma}{\sqrt{n}} \le \mu \le \bar{X}_n + z_{\alpha / 2} \cdot \frac{\sigma}{\sqrt{n}})
\end{align*}
$$

_We first multiply by the denominator and then subtract the sample mean. The
multiplication by $-1$ lead to a change in the direction of the inequalitie, as
well as negates all terms at the inveral boundaries._

<br />

We have found that $L_n = \bar{X}_n - z_{\alpha / 2} \cdot \frac{\sigma}{\sqrt{n}}$
and, similarly, $U_n = \bar{X}_n + z_{\alpha / 2} \cdot \frac{\sigma}{\sqrt{n}})$.

To compute the confidence interval on a specific dataset $(x_1, \ldots, x_n)$ ,
we compute the interval estimates $l_n$ and $u_n$, which will yield the
following $100(1-\alpha)\%$ confidence interval.

$$
(l_n, u_n) =
\left(
\bar{x_n} - z_{\alpha / 2} \cdot \frac{\sigma}{\sqrt{n}},
\bar{x_n} + z_{\alpha / 2} \cdot \frac{\sigma}{\sqrt{n}},
\right)
$$

For example, to compute the 95% CI for the mean of a dataset that is assumed to
be normally distributed with known standard deviation $\sigma$, we compute the
sample mean $\bar{x}_n$ and then compute the margins as $\pm z_{0.05 / 2} \cdot
\frac{\sigma}{\sqrt{n}}$, which yields the following interval.

$$
\begin{align*}
(\bar{x}_n -z_{0.05/2} \cdot \frac{\sigma}{\sqrt{n}}, \bar{x}_n - z\_{0.05/2} \cdot \frac{\sigma}{\sqrt{n}})\\
= (\bar{x}_n - 1.96\cdot \frac{\sigma}{\sqrt{n}}, \bar{x}_n + 1.96 \cdot \frac{\sigma}{\sqrt{n}})
\end{align*}
$$

We can do this programmatically in `R` like this:

```r
# base r implementation
ci.for.mean <- function(sample, confidence.level, sd) {
    n <- length(sample)
    sample.mean <- mean(sample)

    # critical values
    upper <- qnorm((1-confidence.level)/2, lower.tail = F)
    lower <- qnorm((1-confidence.level)/2)

    c(sample.mean - upper * (sd / sqrt(n)),
      sample.mean - lower * (sd / sqrt(n)))
}

test <- rnorm(1000, mean=10, sd=5)
ci.for.mean(test, 0.9, 5)
```

### Case 2: CI for Mean of Normal Data with unknown variance

---

However, if we do not know the true standard deviation/ variance of the random
variable describing the sample mean, we cannot properly normalise the random
variable. Clearly, our best guess for the true standard deviation is the sample
standard deviation $S_n$, which is the unbiased estimator for $\sigma$. The
estimator is defined as,

$$
S_n = \sqrt{\frac{1}{N-1}\sum_i^N (x_i - \bar{x}_n)^2}
$$

, and yields the sample standard deviation $s_n$ for a specific dataset. We can
use $S_n$ to normalise the sample mean $\bar{X}_n$. We denote the normalised
random variable as $T$:

$$
T = \frac{\bar{X}_n - \mu}{S_n / \sqrt{n}}
$$

$T$ is often referred to as the studentised mean, and it no longer depends on
the true value of $\sigma$. However, now it also does not follow a standard
normal distribution, but a so-called **t-distribution** (a special kind of
normal distribution) with $n-1$ degrees of freedoms. We write that $T \sim
T(n-1)$. Again, we can write an interval for this random variable as:

$$
P(c_l \le T \le c_u) = 1 - \alpha
$$

And the critical values $c_l$ and $c_u$ now becomes the critical values from the
t-distribution. We will denote them as $-t_{n-1, \alpha / 2}$ for $c_l$ and
$t_{n-1, \alpha / 2}$ for $c_l$ for $c_u$, respectively. Following the exact same
computational steps as before, we can construct a $100(1-\alpha)\%$ confidence
interval for the mean for normal data with unknown variance through the interval
estimators $L_n$ and $U_n$,

$$
\begin{align*}
   L_n = \bar{X_n} - c_u \cdot \frac{s_n}{\sqrt{n}} \\
   U_n = \bar{X_n} - c_l \cdot \frac{s_n}{\sqrt{n}}
\end{align*}
$$

and their realisation $l_n$ and $u_n$ to compute the concrete CI given data and
estimates for the mean and standard deviation.

$$
(\bar{x}_n -t_{n-1,a/2} \cdot \frac{s_n}{\sqrt{n}}, \bar{x}_n + t_{n-1, a/2} \cdot \frac{s_n}{\sqrt{n}})
$$

<br />

In R we can compute the following:

```r
# base r implementation
ci.for.mean <-  function(sample, confidence.level) {
    n <- length(sample)
    sample.mean <- mean(sample)
    sample.sd <- sd(sample)

    upper <- qt((1-confidence.level)/2, df=n-1, lower.tail = F)
    lower <- qt((1-confidence.level)/2, df=n-1)

    c(sample.mean - upper * (sample.sd / sqrt(n)),
      sample.mean - lower * (sample.sd / sqrt(n)))
}

test <- rnorm(1000, mean=10, sd=5)
ci.for.mean(test, 0.9)
```

### Case 3: CI for the Mean for Arbitrary Distribution using Bootstrapping

---

The above two plug-and-go-formulas that we derived are both assuming normality
of the data, i.e. each data point originates from a normal distribution.
Although the CLT allows us to transform any random variable into a standard
normal distribution for large $n$, it might be a better approach to use the
technique of bootstrapping to compute the confidence interval.

In that, last and most general case, we suppose some given dataset
$x_1, x_2, ..., x_n$ as a realisation of a random sample from some underlying
theoretical distribution with CDF $F$, and we want to construct a confidence
interval for its (unknown) expectation $\mu$.

We can still assume that the sample mean is a good _point estimate_ for the true
expectation $\mu$. Now, we want to use the bootstrap principle to construct the
interval for $\mu$ around $\bar{x}$.

1. Estimate the true distribution $F$ through the empirical distribution
   function $F_n$ (_empirical)_
2. Generate a bootstrap dataset $x_1^*, x_2^*,
   ..., x_n^*$ from $F_n$
3. Compute the studentised mean from the bootstrap dataset, such that
   $$
   t^*=\frac{\bar{x}_n^*-\bar{x}_n}{s_n^*/\sqrt{n}}
   $$
4. Repeat steps 2 and 3 $k$ times. This will yield a vector of $k$ studentised
   means, whose distribution is described as
   $T^*=\frac{X_n^*-\mu^*}{S_n^*/\sqrt{n}}$ and resembles the true distribution
   of the studentised mean. From this we can find the confidence interval, such
   that the critical values are the values for which $\alpha / 2$ are smaller
   and bigger than the value.
5. We then plug back into the regular formula for the confidence interval, such
   that:
   $$
   (\bar{x}_n-c_u^*\frac{s_n}{\sqrt{n}},\bar{x}_n+c_u^*\frac{s_n}{\sqrt{n}})
   $$

We can implement the above algorihtm in R as follows:

```r
# base r implementation
bootstrapped.ci.for.mean <- function(sample, confidence.level, k=500) {
    # compute initial values of interest
    n <- length(sample)
    sample.mean <- mean(sample)
    sample.sd <- sd(sample)

    # initialise bootstrap process (k times)
    bootstrapped.studentised.means <- c()
    for (i in 1:k) {
        # sample with replacement (analogus to draw random samples from F^*)
        bootstrapped.sample <- sample(sample, size=n, replace=T)
        # computed studentised mean on each iteration and append to vector
        studentised.mean <- (mean(bootstrapped.sample) - sample.mean)  / (sd(bootstrapped.sample)/sqrt(n))
        bootstrapped.studentised.means <- c(bootstrapped.studentised.means, studentised.mean)
    }
    # after bootstrapping we have T* given through the ECDF of bootstrapped.studentised.means
    # we can find, thus, find the bootstrapped critical values through quantile()
    upper <- as.numeric(quantile(bootstrapped.studentised.means, 1-((1- confidence.level) / 2)))
    lower <- as.numeric(quantile(bootstrapped.studentised.means, (1- confidence.level) / 2))

    # and then use the bootstrapped critical values to report the CI
    c(sample.mean - upper * (sample.sd / sqrt(n)),
      sample.mean - lower * (sample.sd / sqrt(n)))
}

# example
test <- rnorm(1000, mean=10, sd=10)
print(bootstrapped.ci.for.mean(test, 0.9))
```
