---
title: Basis and Dimension
description:
course: Linear Algebra & Optimisation
tags: []
published: 2022-09-29
lastEdited: 2022-09-29
---

Last week we have first looked at the abstract definition of vector spaces and spanning sets. This week we are going to introduce the notion of basis
and dimension, which are directly related to sets that span vector spaces.

## Basis

---

In short, a basis for a vector space is a minimal spanning set of vectors, i.e., a set that spans all of the vector space, and which is minimal in the
sense that removing one vector will break this property. Mathematically, we say that a set of vectors $S=\{s_1, s_2, ..., s_n\}$ is a basis a vector
space $V$, iff.

1. $S$ spans $V$ (i.e. every $v\in V$ can be written as a linear combination of the vectors in $S$)
2. $S$ is linearly independent (i.e. no vector in $S$ can be written as a linear combination of the remaining vectors in $S$)

### Proving Bases

---

In order to prove that a set of vectors $S$ is indeed a basis for some vector space $V$, we need to prove the two conditions.

1. **Showing that $S$ spans $V$**

   Last week, we have already seen, how we can show if a set of vectors spans some vector space. Let $s_1, ..., s_n$ be the vectors in $S$ and
   $c_1,...,c_n\in\R$ be scalars. Then we wish to find every $u\in V$ as a linear combination of the vectors is $S$, so:

   $$
   c_1s_1+c_2s_2+...+c_ns_n=u
   $$

   For spans in $\R^n$ dimensional space, we can combine the span vectors into a coefficient matrix $A$, leading to the matrix equation:

   $$
   A\vec{c}=\vec{u}
   $$

   This matrix equation has a unique set of scalars $\vec{c}$ for each $\vec{u}$ iff. the matrix $A$ is invertible, since the any solution can be
   written as $A^{-1}\vec{u}$. Thus, we compute

   $$
   det(A)\ne0
   $$

   To show, that the set $S$ spans $V$.

2. **$S$ is linearly independent**

   For a set of vectors (which is also a span of $V$) to be linearly independent means that writing any vector in the vector space has a unique
   solution. For simplicity, we choose the null vector, so we must show that

   $$
   c_1s_1+...+c_ns_n=0
   $$

   only has the trivial solution $c_1=...=c_n=0$. So, we again write the SLE as in an augmented matrix and reduce into RREF to show the above
   statement. If this is not possible, we have a linearly dependent set, in which one or more vectors can be written as a linear combination of other
   vectors.

   _Note, that this is a homogeneous SLE $Ax=0$. By definition, this system always has the trivial solution $x=0$ (and is therefore consistent, since
   it can never not be solvable). Thus, linear dependent sets lead to are systems with infinitely many solutions. The infinite solution set, shows us
   how we can represent any vector in the SLE has a linear combination of the other._

## Dimension

---

Once, you have proven some standard and non-standard bases for common vector spaces, you may notice, that for a single vector space the number of
vectors inside all bases is the same. This is a corollary of the above Theorem: If $S$ is a basis of $V$, then all bases of $V$ have the same number
of vectors $|S|$. This unique property, is referred to as the dimension of a vector space:

If a vector space $V$ has a basis consisting of $n$ vectors, then the number $n$ is the dimension of $V$, denoted by $dim(V) = n$. When $V$ consists
of the zero vector alone, the dimension of $V$ is defined as zero.

> Note, that if you know the dimension of a vector space $|V|=n$, proving if a set if a basis simplifies. If $|S|=n$, then it now suffices to show
> either, that the vectors are linearly independent (because then they also have to span $V$), or show that they span $V$ (because $n$ vectors
> couldn’t span $V$ if they are not linearly independent). Thus, either one of the two conditions implies the other if the number of vectors in $S$ is
> equal to the dimension of the vector space.

### Dimension of Common Vector Spaces

---

For the common vector spaces, the dimensions are the following:

- $dim(\R^n)=n$
- $dim(\R^{m\times n})=mn$
- $dim(P_n)=n+1$

### Dimension of Subspaces

---

If $W$ is a subspace of a vector space $V$ that has dimension $n$, then $dim(W) \le n$ examples show a technique for finding the dimension of a
subspace. Basically, you determine the dimension by finding a set of linearly independent vectors that spans the subspace. This set is a basis for the
subspace, and the dimension of the subspace is the number of vectors in the basis.

## Subspaces of Matrices

---

Next we are going to study special kinds of vector spaces that were observed in the study of matrices: the **row space**, **column space** and **null
space** of any matrix $A\in\R^{m\times n}$.

### Row and Column Vectors

---

To understand these special vector spaces, let’s first be clear about what row and column vectors of a matrix are. The idea is quite simple. Any
$m\times n$ matrix $A$ has $m$ rows and $n$ columns. If we view a matrix as a concatenation of one-dimensional vectors, then we can either concatenate
$n$ (vertical) column vectors or $m$ (horizontal) row vectors. To be specific, given the matrix

$$
A=
\begin{bmatrix}
   a_{11} & ... & a_{1n}\\
   \vdots & \ddots & \vdots\\
   a_{m1} &\dots&a_{mn}
\end{bmatrix}
$$

We can either write a set of $m$ row vectors of the form

$$
\text{Row Vectors}=\{r_1, ..., r_n\}=\{\begin{bmatrix}a_{11} & ... & a_{1n}\end{bmatrix}, ..., \begin{bmatrix}a_{m1} & ... & a_{mn}\end{bmatrix}\}
$$

Or as a set of $n$ column vectors of the form

$$
\text{Column Vectors}=\{c_1, ..., c_n\}=\{\begin{bmatrix}a_{11} & ... & a_{m1}\end{bmatrix}^T, ..., \begin{bmatrix}a_{1n} & ... & a_{mn}\end{bmatrix}^T\}
$$

### Row and Column Space

---

Notice, that this is nothing more than splitting apart a two-dimensional array (the matrix) into a set of one-dimensional arrays. Now, we can view
these vectors as spans, meaning that we can talk about the set of vectors that we can compute as a linear combination of either all _row vectors_ and
all _column vectors_. These spans are again vector spaces, and because they stem from the _row_ and _column_ vectors of a matrix, we call the
resulting vector spaces **row** and **column space**, respectively.

$$
R(A)=span(\text{Row Vectors})=span(\{r_1, ..., r_n\})\in R_n
$$

$$
C(A)=span(\text{Column Vectors})=span(\{c_1, ..., c_n\})\in R_m
$$

Notice, that for a $m\times n$ matrix, the row space - often denoted as $R(A)$ - is going to be a subspace of $\R^n$, since each of the $m$ row vector
is of length $n$. Similarly the column space - typically denoted as $C(A)$ - is going to be subspace of $\R^m$, since each of the $n$ column vectors
is of length $m$.

### Basis for the Row and Column Space

---

We are often interested in finding a basis and the dimensionality of a subspace. In the case of matrix subspaces, like the row and column space, this
means that we require the row and column vectors (whose linear combination makes up the subspace) to also be linear independent (according to the rule
of linear independence of vectors to form a basis for a vector space).

**Option 1: Checking Spanning Set for Linear Independence**

---

This means, that in order to find the basis for the row or column space, we need to make sure that all vectors in the span are linear independent. We
can test for linear independence, by writing the following equation:

$$
a_1v_1+a_2v_2+...+a_nv_n=0
$$

_Here, $v_1, ...,v_n$ are the vectors in either the row or column space and $a_1,...,a_n$ are scalars._

To check for independence, we need to show that this equation only has the trivial solution $a_1=...=a_n=0$. If there are more solutions, this means,
that we can write either vector as a linear combination of the others, meaning that we can drop one (or more) vectors to get a valid basis for the
subspace.

**Option 2: Reducing the matrix into REF/ RREF**

---

An alternative approach uses the theorem that for two row-equivalent matrices $A$ and $B$ the row space $R(A)=R(B)$ are equal. Thus, we can also first
reduce the matrix into REF (or RREF, both will yield a valid basis) and then the basis for the row space will be the set of all non-zero rows

_Notice, that this theorem is defined for row-equivalent matrices obtained from transformations on $A$ through elementary row operations_. _Naturally,
we can do the same thing for the column space - either by using elementary column operations (which are defined in the same way just for columns) or
finding the row space for the transpose $A^T$ through regular ERO._

If you have found a couple of row and column spaces of matrices, you will notice that the dimension of the row space is always equal to the dimension
of the column space (no matter the dimensions of the matrix). This finding is summarised in the notion of the rank of a matrix. So, for any matrix $A$
we can find its rank as the dimension of the row space, which is also the dimension of the column space:

$$
rank(A)=dim(R(A))=dim(C(A))
$$

### Null space

---

There is a third subspace associated with any matrix, called the **null space**. The null space of any matrix $A\in \R^{m \times n}$ is defined to be
the solution set to the homogeneous system of linear equations $Ax=0$ (that is simply the set of solutions for the matrix product of $A$ and $x$ is
the zero vector). This set is a subspace of $\R^n$ (since $x\in\R^n$) and is denoted as $N(A)$.

$$
N(A)=\{x\in\R^n| Ax=0\}
$$

The dimension of this subspace is called the nullity of a matrix $A$.

**Relating null spaces to solutions to SLE**

We have seen that the set of solutions to the system of linear equations $Ax=0$ is a subspace, which we called the null space. However, the system of
linear equation $Ax=b$ is not subspace, because it does not include the zero vector (see that for the subspace to include the zero vector, it must
include $x=0$, which means $A0=b$, but $A0=0$, but $b$ can’t be zero since we talk about nonhomogeneous SLE). However, there is a relationship between
the null space (the solution set to $Ax=0$) and the solution set to a non-homogeneous SLE $Ax=b$. Namely, if you reduce a regular augmented matrix
$[A\ \ b]$ into RREF to obtain a solution for $x$, then you either get a single solution vector (in which case the null space is just the null vector)
or you get a solution space, which you can write up in the form $x_p+su_1+...+tu_2$. In this scenario $x_p$ is being referred to as a particular
solution (it is just a vector of constants that is a unique solution to the vector of constants that you were solving for earlier and the linear
combination $su_1+...+tu_2$ is in fact the null space. Thus any solution set of a non-homogeneous SLE is not a subspace in itself but is the null
space added with a vector of constants (called a particular solution of $Ax=b$). We can summarise that for any nonhomogeneous SLE $Ax=b$ the solution
can be written as

$$
x=x_p+x_h
$$

where $x_h$ is the solution space of the homogeneous SLE (so the null space) and $x_p$ a particular solution to the system $Ax=b$ (which would be
different for the system $Ax=b_2$).
