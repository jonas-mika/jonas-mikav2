---
id: lln
title: The Law of Large Numbers
tags: []
published: 03-19-2023
lastEdited: 03-19-2023
---

This week turn our attention to one of the most central - and likewise most
intuitive - laws in statistics relating probability theory to real-world
phenomena. The law of large numbers gives us the statistical foundation to be
able to relate real-world data to properties of theoretical distributions we
have observed.

**Motivation.** Our starting point in the field of probability theory was that
we wanted to be able to acquire tools to deal with phenomena that involve
randomness. We can formalise this randomness as follows: Whenever we observe
some random phenomenon in the real world, we find that performing the same
experiment under seemingly identical circumstances results in different
outcomes. If one, i.e. measures the speed of light - something that we would
expect to not change - we still measure different speeds. Uncontrollable factors
cause somewhat "random" variation.

However, the LNN allows us to be more and more confident in our results, the
more often we perform a specific experiment. For the coin toss, I might be
extremely lucky and only get $H$ on the first five throws, however, given a fair
coin, for 1000 throws, we can almost be certain that the sample mean is very
close to the true mean of the underlying distribution.

In that sense, the **law or large numbers** is our way of coping with random
variation in random phenomena, which we can eliminate from our measurements by
repetition.

> In probability theory, the law of large numbers (LLN) is a theorem that
> describes the result of performing the same experiment a large number of
> times. According to the law, the **average of the results** obtained from a
> large number of trials should be close to the **expected value** and will tend
> to become closer to the expected value as more trials are performed.

**Proof of Correctness.** To provide statistical reasoning for why the law of
large numbers (LLN) holds, we consider the experiments as a sequence of random
variables $X_1, X_2, ..., X_n$, where $X_i$ is the specific result or outcome of
the $i$th repetition of our experiment. Each of the random variables $X_i$ has
some expectation and standard deviation, which we denote as $\mu$ and $\sigma$.
We restrict ourselves to the situation where the experimental conditions and the
subsequent experiments are identical, ie. we don't change the conditions of the
experiment, and the outcome of one experiment doesn't influence the others, ie.
we assume independence. We call such a sequence an **independent and identically
distributed (i.i.d) sequence**.

We can compute the average of this sequence through our standard method of
averaging, i.e. we sum over the realisation of all random variables and divide
by the number of repetitions:

$$
\bar{X_n} = \frac{X_1 + X_2 + ... +X_n}{n}
$$

<br />

**Expectation.** $\bar{X}_n$ in itself is a random variable, since for another
$n$ repetitions, we would assume a different value for $\bar{X}_n$. We can
compute the expectation and variance of this new random variable.

$$
\begin{align}
E[\bar{X}_n]
&= E[\frac{1}{n}(X_1+X_2+...+X_n)] \\
&= \frac{1}{n}E[X_1+X_2+...+X_n] \\
&= \frac{1}{n}( E[X_1]+E[X_2]+...+E[X_n]) \\
&= \frac{1}{n} (\mu + \mu + ... + \mu) \\
&= \frac{n \cdot \mu}{n} = \mu
\end{align}
$$

We see: When performing identical experiments a large number of times, the
expected value of the random experiment is equal to the expectation of each
single random variable. Sometimes there will be values above the expectation,
sometimes below - but if we average over it, we assume to obtain the same value
again.

<br />

**Variance.** Let's have a look at how the variance of this random variable
behaves:

$$
\begin{align}
Var[\bar{X}_n]
&= Var[\frac{1}{n}(X_1+X_2+...+X_n)] \\
&= \frac{1}{n^2}Var[X_1+X_2+...+X_n] \\
&= \frac{1}{n^2}(Var[X_1]+Var[X_2]+...+Var[X_n]) \\
&= \frac{1}{n^2} (\sigma^2 + \sigma^2 + ... + \sigma^2) \\
&= \frac{n \cdot \sigma^2}{n^2} = \frac{\sigma^2}{n}
\end{align}
$$

For the variance, we have found an even more interesting property. The variance
in the average of the realisation of a sequence of random variables, decreases
the bigger the sequence, i.e. its standard deviation is less than that of a
single realisation by a factor of $\sqrt{n}$. This property of the variance is
what we use to prove the law of large numbers: The more often we repeat some
experiment, the smaller the spread of the averaged results.

<br />

**Chebyshev's Inequality.** We have shown that increasing values of $n$ in our
random variable $\bar{X}_n$, which averages over the results of some random
variable, results in a decreasing variance. If we were to plot the distribution
of our new random variable for increasing values of $n$, we would observe a
contraction of the probability mass around the expectation (less variance â†’ less
spread out probability mass). We will now learn about a tool, that will provide
us with the final step to prove the law of large numbers formally.

Chebyshev's inequality is a way to quantify the bound of the probability that
any random variable $Y$ is outside some symmetric interval of width $2a$ around
its expectation, i.e. outside the interval $[E[Y]-a, E[Y]+a]$. Chebyshev proved
the following upper bound for an arbitrary distribution $Y$:

$$
P(|Y-E[Y]|\gt a) \le \frac{1}{a^2}Var[Y]
$$

We should read this inequality as follows: The probability that the realisation
of some random variable $Y$ deviates more than $a$ from its mean is (always)
lower than the $1/a^2\ Var[Y]$. This is somewhat intuitive since we would expect
this probability to get lower for bigger intervals (bigger values of $a$) and
higher for higher variances.

_Note, that Chebyshev's inequality is in a sense a generalisation of the
68-95-99.7 rule that only applies to normal distributions. For any distribution
with known expectation and variance, we can compute a lower bound that the value
is within some range of standard deviations:_

<br />

The Chebyshev Inequality provides us with powerful tools to make statements
about the probability mass of any random variable. If we want to say something
about the probability that our random variable is within a few standard
deviations we can employ Chebyshev's inequality.

<br />

(1) We can rewrite the probability that our $Y$ is within some standard
deviation $k\sigma$, as the counter-event to when $Y$ is outside this interval.

$$
P(|Y-\mu|\lt k\sigma)=1-P(|Y-\mu| \ge k\sigma)
$$

<br />

(2) Replacing the right side of the equation and using Chebshev's inequality
with $a=k\sigma$:

$$
P(|Y-\mu| \lt k\sigma)\ge 1-\frac{Var[Y]}{k^2\sigma^2}=1-\frac{1}{k^2}
$$

<br />

**Final Proof.** We will now bring together both discoveries and consider the
Chebyshev inequality for our random variable $\bar{X}_n$ modelling the average
over $n$ independent realisation of identical experiments, for which we already
found $E[\bar{X}_n]=\mu$ and $Var[\bar{X}_n]=\sigma^2/n$.

We are interested in the probability that our random variable is realised and
outside some arbitrary interval $\epsilon$:

$$
P(|\bar{X}_n-\mu|\gt\epsilon)\le\frac{1}{\epsilon^2}Var[\bar{X}_n]=\frac{\sigma^2}{n\epsilon^2}
$$

This inequality depends on the number of repetitions $n$. However, letting $n$
go to infinity, i.e. by taking the limit, we can observe the following, which is
the formal proof for the law of large numbers:

$$
lim_{n \to \infty}P(|\bar{X}_n-\mu|\gt \epsilon)=0
$$
