---
id: simulation
title: Simulation
tags: []
published: 02-28-2023
lastEdited: 02-28-2023
---

In many areas of science, technology, government, and business, stochastic
simulation is used to gain understanding of some part of reality that would
otherwise be infeasible to study in the real world or with mathematical
analysis, since they are simply too complex.

Stochastic simulation is the process of generating values (drawing repeated,
independent random samples) for some defined random variable $X$, thus mimicking
outcomes for the whole system.

We can use this both for simulating difficult probabilistic model and to verify
theoretical concepts, such as the expectation, mean, standard deviation using
the law of large numbers.

### Pseudo-Random Number Generators

---

Simulations are almost always done using computers, since we require high
amounts of _sample_ through many runs (iterations of realising a random
variable). This is usually done a _(pseudo) random number generator_ that is
natively implemented in computers.

A call to a _pseudo random number generator_ return a random number be 0 and 1,
which mimics a realisation of the uniform distribution $U(0,1)$. In `R` we can
do this through:

```r
runif(1)
```

Fascinatingly, we can use this source of uniform pseudo randomness to construct
any complex random variable.

### Simulating Discrete Random Variables

---

We know that discrete random variables can only attain values within a finite
(or countably infinite) sequence of values, captured by the range $R_X$ of the
discrete random variable $X$. Imagine there are $|R_X|=n$ possible outcomes
$\{x_i, ..., x_n\}$ and the PDF $f_X$ captures the probabilities $p_1, ..., p_n$
of each possible outcome. To sample from this distribution we partition the
range $[0,1]$ into $n$ chunks, where the $i$-th chunk captures the interval
$[p_i, p_{i+1}]$. Thus, more probable outcomes in the range have larger chunks.
Now, to sample a value, simply generate $u_i \sim U(0,1)$. This will create a
random samples from the discrete random variable $X$ for the value $x_i$ if
$u_i \in [p_i, p_{i+1}]$.

Let's see how this looks in practice for the common theoretical distributions.

<br />

**Simulating Bernoulli Random Variables.** Suppose $U$ has a $U(0,1)$. To
construct a $Ber(p)$ random variable $X$ for some $0 \le p \le 1$, we define

$$
X = \begin{cases} 1 \text{ if } U \lt p, \\0 \text{ if } U \ge p\end{cases}
$$

with the probability mass function

$P(X=1) = P(U \lt p) = p$

$P(X=0) = P(U \ge p) = 1-p$

We have thereby used the realisation of our uniform distribution $U(0,1)$ to be
able to simulate the outcomes of Bernoulli experiment.

```r
simulate.bernoulli <- function(p) {
  u <- runif(1)
  if (u <= p) { return(1) }
  else { return(0) }
}
```

<br />

**Simulating Binomial Random Variables.** Again, suppose $U$ has a $U(0,1)$
distribution. To construct a $Bin(n,p)$ variable $X$, modelling the number of
successes $k$ in $n$ independent Bernoulli trials, each with success probability
$p$, we can define, that our realised variable $X$ is the sum of $n$ Bernoulli
trials, such as:

$X = sum(X_1, X_2, ..., X_n)$

where each $X_1, X_2, ..., X_n$ is $Ber(p)$ distributed random variable such
that

$$
X =
\begin{cases}
1 \text{ if } U \lt p, \\
0 \text{ if } U \ge p
\end{cases}
$$

Using this simple construction, we can realise a binomial random variable.

```r
simulate.binomial <- function(n, p) {
  successes <- 0
  for (i in 1:n) {
    ber <- simulate.bernoulli(p)
    if (ber == 1) { successes <- successes + 1 }
  }
  return(successes)
}
```

<br />

**Simulating Geometric Random Variables.** As with the binomial distribution,
suppose $U$ has a $U(0,1)$ distribution. To construct a $Geo(p)$ variable $X$,
modelling the trial of first success $k$ in a sequence of Bernoulli trials, each
with success probability $p$, we can define:

```r
simulate.geometric <- function(p) {
  trial <- 1 # or: failures <- 0
  while (T) {
    u <- runif(1)
    if (u <= p) { return(trial) }
    else { trial <- trial + 1 }
  }
}
```

<br />

**Simulating any Discrete Random Variable.** It is very easy in R to sample from
an arbitrary PMF. To do so, we just define the range $R_X$ of all possible
values our random variable can attain and the PMF for each of these values in
two separate vectors with equal length. We can then use the sample command to
sample from the range according to the probabilities specified in the PMF

```r
# number of heads in two throws of heads
range <- c(0,1,2)
pmf <- c(1/4, 1/2, 1/4)
sample <- sample(range, replace=T, prob=pmf)
```

### Simulating Continuous Random Variables

---

The above method obviously fails on continuous random variables, since instead
of two possible events with probabilities $p$ and $1-p$, we have infinitely many
possible single events with infinitely small probabilities.

To realise continuous random variables, we therefore need another method. We
suppose that for some _continuous random variable $X$_, we know the distribution
function $F$ and assume it to be strictly increasing. Now, we know that for the
function $F: [a,b] \to [0,1]$ maps values to probabilities. The range of the
function must therefore be the interval from 0 to 1, both including. The inverse
function $F_{Inv}: [0,1] \to [a,b]$ obviously reverses this observation. This
function is only defined on the interval 0 to 1, both including. This
observation yields to a sampling technique that strikes out because of its
generality and simplicity. It states: If you know the cumulative distribution
function (CDF) of a probability distribution, then you can always generate a
random sample from that distribution.

We can use this observation to pass in our random values between 0 and 1 that
are drawn (pseudo) randomly from our random number generator that mimics the
behaviour of $U(0,1)$ by passing in the $u \in U$ into the inverse function.
Each time we do this, we draw a random sample from the random variable, whose
probability distribution is described by $F$:

$$
F_{Inv}(u) = x \leftrightarrow F(x)=u
$$

Here $x$ describes a random sample from the distribution with inverse
distribution function $F_{Inv}$.
