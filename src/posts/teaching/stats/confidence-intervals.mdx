---
id: confidence-intervals
title: Confidence Intervals
tags: []
published: 04-24-2023
lastEdited: 04-24-2023
---

So far, we have only dealt with finding the best possible _point estimator_. For
some random sample $x_1, x_2, ..., x_n$ from a sequence of independent,
identically distributed random variables $X_1, X_2, ..., X_n$, we defined an
estimator $T$ as a function of the sample, that outputs a single digit, that is
supposed to estimate a theoretical quantity of the distribution of $X$.

However, especially, when the size of the sample is small, the point estimate
produced by our estimator might be very off (since we are only confident that it
will converge towards the real-value for a large sample size). Thus, we cannot
be sure how close our estimated value is to the real value. The metric in itself
is therefore not robust, because it involves randomness in the samples collected
from the underlying population. Sampling the same amount again, might ie. give a
different mean than the initially computed. (_Single) point estimators_ are
therefore, after all, only estimates.

So far, we have only considered them to be _somewhat close_ to the _real
theoretical value_. In this section, we are trying to _quantify_ what we mean by
_somewhat close_ by introducing the concept of **Confidence Intervals (CI)** and
therefore looking at so-called _interval estimators._

## General Concept

---

For a random sample $x_1, x_2, \ldots, x_n$ from a sequence of i.i.d. random
variables $X_1, X_2, \ldots, X_n$ and some desired model parameter $\theta$, a
confidence interval is an interval $[\hat{\theta}_l,\hat{\theta}_h]$ that is
likely to includes the true value of $\theta$ with some probability $p$.

In interval eestimation, there are two important concepts:

1. **Length of the reported interval $\hat{\theta}_h - \hat{\theta}_l$**

   The length of the reported interval shows the precision with which we can
   estimate the model parameter $\theta$.

2. **Confidence Level**

   The confidence level shows how confident we are about the interval. The
   higher the confidence level, the more certain we are that the true value of
   $\theta$ is within the interval. Commonly, confidence levels are
   reported at a level of 95%, 99% of 99.5%.

_Note, that the two metrics are positively related to each other. That is, the
larger the reported interval, the higher the confidence level, but also the
less precise the interval. Thus, the goal is to find the smallest possible
interval with a critical confidence level._

## Prerequisites

---

In a second we are going to see _three_ plug-and-play formulas for _three_ cases
for constructing the confidence interval for the mean in three different cases.
It is _very easy_ to just use, but not understand, these formulas. My goal here
is to make you understand the theory in-depth, so that you would be able to
derive all formulas yourself if you wanted to. To be able to that, we will have
to revisit some simple examples from probability theory, which are going to be
crucial for understanding interval estimation.

<br />

**Probabilities on Intervals.** Let $X$ be a random variable with CDF
$F_X(x)=P(X \le x)$ and suppose that you are to find an interval $[x_l, x_h]$,
such that $100\cdot(1-a)\%$ of the probability mass of $X$ fall within that
interval. Formally written down, we are searching for 

$$
P(x_l \le X \le x_u) = 1-a
$$

While there are infinitely many ways to choose such an interval, we are
considering the case of a **symmetric** interval. For the inner probability mass
to be $1-a$, the tail probabilities combined have to be $\alpha$. Under the
assumption that we construct a symmetric interval, each tail probability
has to be $\alpha / 2$. We denote these tail probabilities as:

$$
P(X \le x_l) = \alpha / 2, \text{and } 
P(X \ge x_u) = \alpha / 2
$$

Following our definition of the CDF, we can equivalently write

$$
F_X(x_l) = \alpha / 2, \text{and } 
F_X(x_u) = 1 - \alpha / 2
$$

_Note, that for the upper interval we use the complement probabitity to be able
to use the CDF, which by default computes left-tail probabilities. Here, $P(X
\ge x_u) = \alpha / 2$ implies $P(X \le x_u) = 1 - \alpha / 2$._

With this knowledge, we can simply use the inverse of the CDF $F_X^{-1}$ to
compute the interval boundaries $[x_l, x_u]$, as 

$$
x_l = F_X^{-1}(\alpha / 2), \text{and }
x_u = F_X^{-1}(1 - \alpha / 2)
$$


The interval

$$
\left( F_X^{-1}(\alpha / 2),\ F_X^{-1}(1 - \alpha / 2) \right)
$$

, is called a $1-\alpha$ interval for $X$.

<br />

**Critical Values.** Before moving to confidence intervals, we have to get one
more terminology down. It is commonly used in literature when deriving the
intuition for confidence intervals, and so we have to know what it is. It will
make our formula for a $1-\alpha$ interval slightly nicer to read.

A **critical value** $z_p$ of a distribution $Z$ is defined as the value with
right-tail probability $p$

$$
P(Z \ge z_p) = p
$$

Equipped with this definition, we can re-write the interval boundaries. Because
the upper interval boundary $x_u$ has a right-tail probability of $\alpha / 2$
($P(X \ge x_u) = \alpha / 2$), we can call $x_u$, $z_{\alpha / 2}$ for $X$.
Similarly, because the lower interval boundary $x_l$ has a right-tail
probability of $1-\alpha / 2$, we can call $x_l$, $z_{1 - \alpha / 2}$ for $X$.

Therefore, we can re-write the entire $1-\alpha$ interval for $X$ as

$$
\left( z_{1-\alpha / 2}, z_{\alpha / 2}\right)
$$

In R, use can compute the critical value $z_p$ for any distribution $Z$, for
which you have the inverse CDF defined. Assuming, for example, a normal
distribution, we can compute $z_p$ as:

```r
qdist(p, lower.tail=F)
```

<br />

**Assuming Normality.** Now, if we assume normality of the process $X$, that is
$X\sim N(0,1)$, then the CDF and inverse CDF are well defined as $\Phi$ and
$\Phi^{-1}$ respectively. We can exchange the inverse CDF in our computation of
the interval boundaries.

$$
x_l = \Phi_X^{-1}(\alpha / 2), \text{and }
x_u = \Phi_X^{-1}(1 - \alpha / 2)
$$

However, the normal distribution is special because its probability density
function is symmetric, i.e. $f_X(-x) = f_X(x)$. For the CDF, this means that
$1-\Phi(x)=-\Phi(x)$ and for the inverse CDF, that $\Phi_X^{-1}(1 - x) =
-\Phi_X^{-1}(x)$. With $x=\alpha /2$ in the case of creating an interval, we get 

$$
x_l = \Phi_X^{-1}(\alpha / 2), \text{and }
x_u = -\Phi_X^{-1}(\alpha / 2)
$$

Written as right-tail probabilities in the form of critical values we can say
that $z_{1 - \alpha / 2} = -z_{\alpha / 2}$, and so we only need to compute a
single critical value, $z_{\alpha / 2}$, to construct the $1-\alpha$ interval
for $Z$

$$
(-z_{\alpha / 2}, z_{\alpha / 2})
$$

## Confidence Intervals for the Mean

---

With this background knowledge, we can start to compute interval
estimators. In this course we will only construct confidence intervals for the
mean $\mu$ for a distribution $X$. We assume that we are given a realisation
$(x_1, \ldots, x_n)$ from the i.i.d. sequence of random variable $(X_1, \ldots,
X_n)$.

Let's recall that an unbiased ($E[\bar{X}_n]=\mu$) point estimator for the
parameter $\mu$ is the **sample mean** $\bar{X}_n$, which is defined as

$$
\bar{X}_n = \frac{X_1 + \ldots X_n}{n}
$$

We can compute the point estimate $\bar{x}_n$ by plugging a realised dataset
into the above formula. We will now develop a procedure to construct intervals
with different lengths and confidence levels to **quantify the uncertainty** of
our point estimate that arises from the sampling procedure.

### Case 1: CI for Mean of Normal Data with Known Variance

---

In this first - most simple - case, we assume that some given data $x_1, x_2,
..., x_n$ is a realisation of a random variable $X_1, X_2, ..., X_n$ that
follows a normal distribution with known variance $\sigma^2$. Our task is now
to determine the $\alpha \cdot 100\%$ confidence interval for the population
mean, ie. given some data and its variance, how confident can we be that the
mean of the underlying theoretical distribution lies within some interval or
what sized interval can we construct to be $95\%$ confident that it includes
the population mean.

Our model parameter is the expectation $\mu$, which we can estimate through the
point estimator of the sample mean _$\bar{X}_n$. Now, however, we are
interested in the distribution of this random variable, as we want to be able
to find critical values for it. A summation of independent, identically
distributed normal random variables in itself is a normal random variable and
we have already shown, that its $E[\bar{X}_n]=\mu$ and
$Var[\bar{X}_n]=\sigma^2/n$. By the central limit theorem, $\bar{X}_n$ is
normally distributed with $N(\mu, \frac{\sigma^2}{n})$. Using standardisation,
we can transform the distribution into a standard normal distribution ($N(0,1)$)
$(Z=\frac{X-\mu}{\sigma})$:

$$
\frac{\bar{X}_n-\mu}{\sigma / \sqrt{n}}
$$

Now, are searching for the probability that our transformed estimator
$\bar{X}_n$ with $N(0,1)$ lies within some interval $c_l$ and $c_u$, so we can
write:

$$
\begin{align}1- \alpha &= P(c_l \le Z \le c_u) \\ &=P(c_l \lt \frac{\bar{X_n}-\mu}{\sigma / \sqrt{n}} \lt c_u) \\ &= P(c_l \cdot \frac{\sigma}{\sqrt{n}} \lt \bar{X_n}-\mu \lt c_u \cdot \frac{\sigma}{\sqrt{n}})\\ &= \alpha=P(\bar{X_n} - c_u \cdot \frac{\sigma}{\sqrt{n}} \lt \mu \lt \bar{X_n} - c_l \cdot \frac{\sigma}{\sqrt{n}})\end{align}
$$

We first multiply by the denominator and then subtract the sample mean. Since we
need to change signs, $c_u$ and $c_l$ change sites.

We have therefore found our lower and upper bound of the $\alpha$ confidence
level as follows:

$$
(\bar{X_n} - c_u \cdot \frac{\sigma}{\sqrt{n}}, \bar{X_n} - c_l \cdot \frac{\sigma}{\sqrt{n}})
$$

Using this formula, we can compute the $100(1-\alpha)\%$ confidence intervals,
by exchanging the constant to the critical values of the standard normal
distribution, such that $P(Z\ge c_u)=\alpha / 2$ and $P(Z \le c_l)=\alpha
/ 2$
and by noticing that $z_{c_l}=-z_{c_u}$ for the standard normal distribution, we
generally find the plug-and-play formula for the $100(1-a)\%$ confidence
interval

$$
(\bar{x}_n -z_{a/2} \cdot \frac{\sigma}{\sqrt{n}}, \bar{x}_n + z_{a/2} \cdot \frac{\sigma}{\sqrt{n}})
$$

Example of the 95% CI: 

$$
(\bar{x}_n -z_{0.05/2} \cdot \frac{\sigma}{\sqrt{n}}, \bar{x}\_n - z\_{0.05/2} \cdot \frac{\sigma}{\sqrt{n}})\\
\implies (\bar{x}\_n - 1.96\cdot \frac{\sigma}{\sqrt{n}}, \bar{x}\_n + 1.96 \cdot \frac{\sigma}{\sqrt{n}})
$$

```r
# base r implementation
ci.for.mean <- function(sample, confidence.level, sd) {
    n <- length(sample)
    sample.mean <- mean(sample)

    # critical values
    upper <- qnorm((1-confidence.level)/2, lower.tail = F)
    lower <- qnorm((1-confidence.level)/2)

    c(sample.mean - upper * (sd / sqrt(n)),
      sample.mean - lower * (sd / sqrt(n)))
}

test <- rnorm(1000, mean=10, sd=5)
ci.for.mean(test, 0.9, 5)
```

### Case 2: CI for Mean of Normal Data with unknown variance

---

However, if we do not know the true standard deviation/ variance of the random
variable describing the sample mean, we cannot normalise the random variable and
therefore not follow the procedures described above.

But, we can help ourselves by using the estimator $S_n$ for the standard
deviation. This will yield a random variable $T$, that is no longer depended on
knowing the variance. It is often called the studentised mean and it follows a
_t-distribution (special kind of normal distribution)_ with $n-1$ degrees of
freedoms, such that $T\sim T(n-1)$.

$$
T = \frac{\bar{X}_n - \mu}{S_n/\sqrt{n}}
$$

Doing the same computations as before, we arrive at the general formula for the
confidence interval. The only difference is that the formula replaces the, now
unknown parameter $\sigma$, through the sample standard deviation.

$$
(\bar{X_n} - c_u \cdot \frac{s_n}{\sqrt{n}}, \bar{X_n} - c_l \cdot \frac{s_n}{\sqrt{n}})
$$

Using this formula, we can compute the $100(1-\alpha)\%$ confidence intervals,
by exchanging the constant to the critical values of the t-distribution as
given.

$$
(\bar{x}_n -t_{n-1,a/2} \cdot \frac{s_n}{\sqrt{n}}, \bar{x}_n + t_{n-1, a/2} \cdot \frac{s_n}{\sqrt{n}})
$$

In R looks like this:

```r
# base r implementation
ci.for.mean <-  function(sample, confidence.level) {
    n <- length(sample)
    sample.mean <- mean(sample)
    sample.sd <- sd(sample)

    upper <- qt((1-confidence.level)/2, df=n-1, lower.tail = F)
    lower <- qt((1-confidence.level)/2, df=n-1)

    c(sample.mean - upper * (sample.sd / sqrt(n)),
      sample.mean - lower * (sample.sd / sqrt(n)))
}

test <- rnorm(1000, mean=10, sd=5)
ci.for.mean(test, 0.9)
```

### Case 3: CI for the Mean for Arbitrary Distribution using Bootstrapping

---

The above two plug-and-go-formulas that we derived are both assuming normality
of the data, i.e. each data point originates independently from the same normal
distribution. Although the CLT allows us to transform any random variable into a
standard normal distribution for large $n$, it might be a better approach to use
the technique of bootstrapping to compute the confidence interval.

In that, last and most general case, we suppose some given dataset
$x_1, x_2, ..., x_n$ as a realisation of a random sample from some underlying
theoretical distribution with CDF $F$, and we want to construct a confidence
interval for its (unknown) expectation $\mu$.

We can still assume that the sample mean is a good _point estimate_ for the true
expectation $\mu$. Now, we want to use the bootstrap principle to construct the
interval for $\mu$ around $\bar{x}$.

1. Estimate the true distribution $F$ through the empirical distribution
   function $F_n$ (_empirical)_
2. Generate a bootstrap dataset $x_1^*, x_2^*,
   ..., x_n^*$ from $F_n$
3. Compute the studentised mean from the bootstrap dataset, such that
   $$
   t^*=\frac{\bar{x}_n^*-\bar{x}_n}{s_n^*/\sqrt{n}}
   $$
4. Repeat steps 2 and 3 $k$ times. This will yield a vector of $k$ studentised
   means, whose distribution is described as
   $T^*=\frac{X_n^*-\mu^*}{S_n^*/\sqrt{n}}$ and resembles the true distribution
   of the studentised mean. From this we can find the confidence interval, such
   that the critical values are the values for which $\alpha / 2$ are smaller
   and bigger than the value.
5. We then plug back into the regular formula for the confidence interval, such
   that:
   $$
   (\bar{x}_n-c_u^*\frac{s_n}{\sqrt{n}},\bar{x}_n+c_u^*\frac{s_n}{\sqrt{n}})
   $$

We can implement the above algorihtm in R as follows:

```r
# base r implementation
bootstrapped.ci.for.mean <- function(sample, confidence.level, k=500) {
    # compute initial values of interest
    n <- length(sample)
    sample.mean <- mean(sample)
    sample.sd <- sd(sample)

    # initialise bootstrap process (k times)
    bootstrapped.studentised.means <- c()
    for (i in 1:k) {
        # sample with replacement (analogus to draw random samples from F^*)
        bootstrapped.sample <- sample(sample, size=n, replace=T)
        # computed studentised mean on each iteration and append to vector
        studentised.mean <- (mean(bootstrapped.sample) - sample.mean)  / (sd(bootstrapped.sample)/sqrt(n))
        bootstrapped.studentised.means <- c(bootstrapped.studentised.means, studentised.mean)
    }
    # after bootstrapping we have T* given through the ECDF of bootstrapped.studentised.means
    # we can find, thus, find the bootstrapped critical values through quantile()
    upper <- as.numeric(quantile(bootstrapped.studentised.means, 1-((1- confidence.level) / 2)))
    lower <- as.numeric(quantile(bootstrapped.studentised.means, (1- confidence.level) / 2))

    # and then use the bootstrapped critical values to report the CI
    c(sample.mean - upper * (sample.sd / sqrt(n)),
      sample.mean - lower * (sample.sd / sqrt(n)))
}

# example
test <- rnorm(1000, mean=10, sd=10)
print(bootstrapped.ci.for.mean(test, 0.9))
```
