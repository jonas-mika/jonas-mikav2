---
id: algorithmic-analysis
title: Algorithmic Analysis
tags: []
published: 02-14-2022
lastEdited: 02-05-2023
---

Before jumping into the math, let's convince ourselves that being able to
mathematically estimate the running time of our programs is a useful skill to
have.

With computers getting more and more powerful in their computational power
throughout the last decades, they are used to solve increasingly difficult
problems, which often comes in along the need to process (insanely) large
amounts of data, ie. when training machine learning models on huge corpora of
natural language data. This invariably leads to the following two questions:

- **How long will my program take?**

- **How much space does my program take up during computation?**

For small inputs, these questions are more or less irrelevant, since the high
processing power of computers makes the difference in ie. sorting a list of 10
or 50 numbers, unnoticeably small. The larger the input, the more relevant these
questions become however, and not rarely, decide between whether or not a
problem is feasible at all.

Fast solutions to problems are good, sure. But why do I need to analyse my
runtime prior to execution. Well, you don't have to. You can just run your
program and if the input is small enough and/ or your solution is fast, you can
empirically feel, how fast your algorithm runs. What if your program is
seemingly not coming to an end? This is when it becomes problematic to treat
runtime as a black box. In these cases, where we are solving difficult problems
on huge problem sizes, it is _incredibly_ valuable to be able to quantify how
fast we expect our program to terminate with the correct solution. Only using
this, we are able to talk about the performance of algorithms in a theoretical
way and not only by empirically observing (or _not observing_, when the problem
sizes grow too large for our local computers to process)

## Definition

---

The above considerations give way for the field of computer science, called
**analysis of algorithms.**

It describes the process of finding the computational complexity of algorithms –
the amount of time, storage, or other resources needed to execute them. Usually,
this involves determining a function that relates the length of an algorithm's
input to the number of steps it takes (its time complexity) or the amount of
storage it uses (its space complexity). An algorithm is said to be efficient,
when this function's values are small, or grow slowly compared to the growth of
the size of the input data.

Being able to analyse algorithms is a fundamental skill indispensable for every
software developer for several reasons:

1. **Predict Performance** (Providing Performance Guarantees)

   We need to be able to predict the performance of our algorithms on different
   kinds of inputs, in order to be able to decide on whether or not to use the
   algorithm at hand, to solve a specific problem.

2. **Compare Algorithms**

   Only the analysis of algorithms allows us to compare different algorithms for
   the same problem, and decide which algorithm performs best for the problem at
   hand.

## The Scientific Method (Empirical Analysis)

---

We know, that we are interested in understanding our algorithm's runtime on
increasingly big inputs, but how can we assess this? The most intuitive
approach, of just measuring the duration of computation for different sizes of
input, is indeed a valid method of estimating algorithm's runtime known as the
**scientific method**. It is a form of empirically analysis of a natural
phenomena - in this case the computer.

### Steps of Scientific Method

---

The very same approach that scientists use to understand the natural world is
effective for studying the run-time of our algorithms:

1. *Observe* some feature of the natural world (here: the computer\* itself),
   generally with precise measurements.
2. *Hypothesise* a model that is consistent with the observations.
3. *Predict* events using the hypothesis.
4. *Verify* the predictions by making further observations.
5. *Validate* by repeating until the hypothesis and observations agree.

The experiments we design must be _reproducible_ and the hypotheses that we
formulate must be _falsifiable._

### Observe Run-Time of Algorithms

---

In order to empirically analyse the running time of our algorithms, we must
quantify their running time, ie. we must measure the amount of milliseconds,
seconds, minutes or even hours that our algorithm takes to compute.

We do such observations by simply measuring their running time on increasingly
big input sizes. To acknowledge for measurement errors (ie. background
computation the hardware is doing), one might perform the same experiment
several times and average over the results, in order to get more precise
measures.

```python
# measure runtime within python
from timeit import default_timer # standard library

def algorithm():
  ...

start = default_timer()
algorithm()
print(f"Time elapsed: {default_timer() - start}")
```

```bash
# runtime from cli
time algorithm.py < input.in
```

## The Mathematical Method

---

As we have seen, software profiling techniques can be used to measure an
algorithm's run-time in practice, and give a very intuitive feeling for the
runtime of the algorithm. However, the scientific method cannot provide timing
data for infinitely many possible inputs; the latter can only be achieved by the
theoretical methods of run-time analysis and is therefore more robust.

We will therefore now discover another way of epistemology that is not based on
empirical analysis, but on mathematically analysis the algorithm at hand.

### Frequency and Cost

---

In the early days of computer science, _D.E. Knuth_, postulated, that, despite
all of the complicating factors in understanding running time (ie. noise and
different performances on different machines), it is possible, in principle, to
build a mathematical model to describe the running time of any program. The
major insight is quite simple:

The total running time of an algorithm, is determined by two factors:

1. **Single Statement Execution Cost** (_machine-dependent_)

   The cost of executing each statement.

2. **Execution Frequency** (_dependent on algorithm and input_)

   The frequency of execution of each statement.

Since the former is a property of the hardware, compiler, operating system and
other external factors, the frequency of executing statements, is solely
determined by the source code and totally independent from the machine. It
therefore comes natural, that we will use this second property to approximate a
mathematical model to describe the running time on different input sizes. Let's
consider counting the frequency of each statement in the code that is
calculating the amount of zeros in a list:

Consider ie. the following `Python` code snippet that is counting the number of
0s for some array.

```python
# count 0 in list in python (count triples that count to 0)
def count_zeros(arr):
  n = len(arr)
  count = 0
  for i in range(n): # for(int i = 0; i<n; i++) {}
    if arr[i] == 0:
      count += 1
```

We arrive at the following counts for single statement executions:

- Variable Declaration: 3
- Assignment Operation: 3
- Array Access: N
- Boolean Comparison : 2N+1
- Increment : N-2N

If we assume **uniformity** in cost, we can simply look at the most frequent
operations and approximate the asymptotic running time of the entire algorithm
with a single operation. It in example becomes negligible that we declare and
assign 3 variables in the algorithm, when we are counting the number of zeros in
a list of a billions numbers, since ie. the number of array accesses grows with
input, while the variable declaration stay constant.

This is a first simplification to adjust the running time. Instead of summing
over all operation, we choose the most frequent operation (which most often
occurs in the 'inner loop' of an algorithm) and approximate the entire running
time with this operation.

However, we can further simplify the running time.

## Modelling Runtime with Functions

---

From the mathematical model, we can construct a function for the runtime of the
algorithm by summing over the frequency of each single statement operation
(again: assuming uniformity of cost of operations). This function tells us
precisely, how (relatively to the machine specification) will take for some
input size.

We call such functions, that relate input size (usually denoted as $N$) to
runtime, **order-of-growth** functions. These order-of-growth functions, however
can get very tedious to deal with, if we don't find ways to simplify them
further.

Specifically, we are interested in a form of the function that is both _easy_ to
comprehend and deal with, but does not abstract too much information away, so
that we are still able to compare the runtime of two algorithms through their
order-of-growth functions.

For this purpose, we use the mathematical of _limits_, and more specifically
_asymptotic notation_. As we will see, this abstraction allows us to be able to
talk about the relative growth of the runtime when input sizes get very large,
which seems to be a legit assumption to make, since we are usually only
interested in (differences of) runtime for large inputs sizes.

## Equality and Bound of Functions

---

Suppose I have modelled the running time of two algorithms, in two mathematical
functions $f$ and $g$, which both depend on the size of my input $N$. What
properties of these two functions am I interested in? Well, I ie. would like to
be able that their running time is equivalent. Likewise, it would be nice to be
able to mathematically express that algorithm 1 runs faster than algorithm 2.

How might I achieve this? Well, mathematics gives us tools to talk about the
equivalence of functions. Specifically, two functions are said to be equivalent
iff. $f(x) = g(x)$ for all $x$ and we say that $f$ is bounded by $g$ iff.
$f(x) \lt g(x)$ for all $x$. Is this useful? Not really. These strict
definitions of equivalence and bounding of functions are too strict to be useful
when talking about functions.

Consider ie. these two runtime functions

$$
f(N) = N+\frac{1}{1000}\\
g(N) = N
$$

The fact that in algorithm 1 (modelled through $f$) we seem to have one
single-statement operation more, than in algorithm 2, would force us to conclude
that $f(N)\ne g(N)$, and thus their runtime is not equivalent. Which, in theory
is correct, but in practice, is so insignificant, that we would like to be able
to say that these are _roughly_ equal.

Likewise, consider these two runtime functions

$$
f(N) = N^2\\
g(N) = N^3
$$

Algorithm 1, whose runtime is modelled through $f$ is quadratic, while
algorithms 2 runtime is cubic. Clearly, we would like to conclude, that $f$ runs
a lot faster than $g$ and is therefore bound by $g$. Our math tools so far do
not allow us to make this assumption, since $f(1)=g(1)$, and therefore
$f\nless
g$ for all $x$.

Clearly, we need to advance our toolset of mathematically talking about
relationships between functions. These new tools should loosen the strictness of
equality and bounded to a level of granularity that is both precise enough to be
interesting and coarse enough to be useful. Note, that we already have a
definition for exactly these concepts.

### Enhancement 1: Asymptotic Equality (Tilde-Notation)

---

Tilde notation is our first enhancement of our toolbox to compare
order-of-growth-functions. We denote that two functions are _asymptotically
equal_ (also: _tilde-equal_), st. $f(x) \sim g(x)$ (abbreviatd to $f \sim g$),
iff. the following conditions holds:

$lim_{n \to \infty} \frac{f(n)}{g(n)} = 1$

Asymptotic equivalence can be usefully interpreted as a fuzzy version of the
standard equality. By looking at the limits for both functions and comparing if
these are equivalent, we are abstracting lower-order terms in both functions,
only focusing on these parts of the functions (operations in our algorithms)
that will take most time when our inputs get huge.

While we disregard lower-order terms, we do keep attention to factors in front
of the highest-order term. Look at these examples to get familiar with
tilde-notation:

1. $n+1 \sim n$, since $lim_{n \to \infty} \frac{n+1}{n} = 1$

2. $n²-n+5 \sim n^2$, since $lim_{n \to \infty} \frac{n² + n -5}{n^2} = 1$, but

3. $2n \nsim n$, since $lim_{n \to \infty} \frac{2n}{n}=2$

Asymptotic equivalence has the following properties:

**Symmetric: $f \sim g$**, if and only if $g \sim f$ (similar to regular
equality)

**Reflexive: $f \sim f$** (a function itself is always asymptotically equal to
itself)

**Transitive:** If $f\sim g$ and $g\sim h$, then $f \sim h$

### Enhancement 2: Asymptotic Bound (Big-O-Notation)

---

Now, that we have a fuzzy version of equivalence with tilde-notation, why not
also have a fuzzy version of talkig about some function bounding some other
function. We will use the same idea of looking at the limits of both functions,
and say that some function $f$ is asymptotically bound to some other function
$g$, iff.

$$
lim_{N \to \infty}\frac{f(N)}{g(N} ≠ \infty
$$

We denote, that $f(x)$ is $O(g(x))$, or $f(x)=O(g(x))$ (abbreviated $f=O(g)$).
From the mathematical definition it becomes clear, that Big-O notation not only
disregards lower order terms, but also stops carign for constant factors in
front of the highest-order term. Some examples are:

1. $n+1 = O(n)$, since $lim_{n \to \infty} \frac{n+1}{n} = 1 \ne \infty$

2. $2n = O(n)$, since $lim_{n \to \infty} \frac{2n}{n} = 2 \ne \infty$, but

3. $n² - n + 5 ≠ O(n)$, since $lim_{n \to \infty} \frac{n² - n +5}{n} = \infty$,
   however the other way would work.

\*Note that mathematically $n = O(n² - n +5)$, since $f$ is asymptotically bound
by $g$, meaning that $f$ is smaller than $g$ for large inputs. In the context of
the analysis of algorithms however, this is not very interesting, since $n$ is
also bound by $O(n)$, making the observation that it is bound by any other
higher-order term redundant and uninteresting.

Asymptotic Bound has the following properties:

**Non-Symmetric:** If $f=O(g)$, then $g$ cannot not be $O(f)$ (except for
asymptotically equal functions)

**Reflexive: $f = O(f)$** (a function is always asymptotically bound by itself)

**Transitive:** If $f=O(g)$ and $g = O(h)$, then $f=O(h)$ (if f is
asymptotically bound by g and g by h, then f is also asymptotically bound by h)

_Also note that two functions that are asymptotically equal, ie. $f \sim g$,
they are also asymptotically bound, ie. $f = O(g)$, but not vice versa._

## Typical Order-of-Growth Functions

---

**Constant**: $O(1)$ (Arithmetic Operations, Initialising Variables, Array
Accesses)

**Logarithmic**: $O(log\ N)$ (Divide in Half (Binary Search))

**Linear**: $O(N)$ (Loop)

**Linearithmic**: $O(N\ log\ N)$ (Divide and Conquer Algorithms)

**Quadratic**: $O(N^2)$ (Double Nested Loop)

**Cubic**: $O(N^3)$ (Triple Nested Loop)

**Exponential** $O(c^N)$ (_Not encountered in course_)

## Exercise Solutions

---

### 1.4.5 Tilde-Approximation

---

a. $N+1 \sim N$, since $lim_{N \to \infty} \frac{N+1}{N} = 1$

b. $1+\frac{1}{N} \sim 1$, since $lim_{N \to \infty} \frac{1 + 1/N}{1} = 1$

c. $(1+1/N)(1+2/N) \sim 1$, since
$lim_{N \to \infty} \frac{1+2/N+1/N+2/N^2}{1} = 1$

d. $2N^3 - 15N^2 + N \sim 2N^3$, since
$lim_{N \to \infty} \frac{2N^3 - 15N^2 + N}{2N^3} = 1$

### 1.4.6

---

```python
sum = 0
n = N
while n > 0:
  for i in range (0 , n):
    sum += 1
  n //= 2
```

The following code snippet runs for-loops in the range of N, and within each
iterations halves N, st. that next for-loop runs over half the number of
elements. This means, that in the first iteration, we iterate over $N$ elements,
in the second over $N/2$, then $N/4$ and so forth. We get the algebraic sequence

$$
N+\frac{N}{2}+\frac{N}{4}+\cdots+2+1
$$

It follows from the geometric sequence, that this is $\sim 2N$, which is $O(N)$.
Thus, this algorithm runs in asymptotic _linear_ time.

```python
sum = 0
i = 1
while i < N:
  for j in range (0 , i):
    sum += 1
  i *= 2
```

This algorithm initialises a variable $i=1$ and then runs a loop from 0 to
(exclusive) $i$ until $i$ is larger or equal to some set $N$. In each iteration,
$i$ is being doubled. Thus, we get a sequence like so:

$$
1 + 2 + \hdots + \frac{N}{2}
$$

Note, that the loop never runs over all $N$, because of the strict less
equal-condition in the outer while loop. Thus, the runtime is $\sim N$ and
$O(N)$. The algorithm therefore also runs in linear time.

```python
sum = 0
i = 1
while i < N:
  for j in range (0 , N):
    sum += 1
  i *= 2
```

This code snippet looks similar to the one above, with the difference, that we
in the inner loop we are iterating over a constant of $N$ elements during every
execution of the while loop. Since at each step, we are doubling $i$, we can
expect the outer loop to execute $lg(N)$ times and thus our total running time
amounts to $O(N\ lg\ N)$.

### 1.4.10 Binary Search

---

We can adjust binary search to return the lowest index of the element we are
searching for from a sorted array. To do this, we leave most of the algorithm as
is, ie. initialise `low` and `high` pointers and recursively searching whether
the search item is in the right or left half and updating the pointer
accordingly. The key difference is now, that once we have found the search item,
ie. `arr[m]==x`, we cannot terminate the algorithm, but we must search the
entire left half and return the smallest index that still contains the element.
Simply linearly searching from $m$ in decreasing order, however, has a
worst-case of $O(N)$, linear time, in the case of an array of all duplicates.
Instead, we can start a another recursive call within the lower half in order to
find the smallest possible index.

```python
def bs_recur(arr, x, l, h):
  if l > h:
    return None

  m = (l+h) // 2

  if arr[m] < x:
    return bs_recur(arr, x, m+1, h)
  elif arr[m] > x:
    return bs_recur(arr, x, l, m-1)
  else:
    smallest_idx = bs_recur(arr, x, l, m-1)
    return smallest_idx if smallest_idx else m
```

```python
def bs(arr, x=None):
  # O(lg N) on sorted arr
  arr = sorted(arr)
  print(arr)
  N = len(arr)
  l, h = 0, N-1

  while l <= h:
    m = (l+h) // 2
    if  arr[m] < x:
      l = m+1
    elif arr[m] > x:
      h = m-1
    else:
      for j in range(l, m):
        if arr[j] == x:
          return j
      return m
  return None
```

### 1.4.12 Algorithm Design

---

Given two sorted arrays `a` and `b` we can all elements that appear in both
arrays (intersection) using the following idea: Initialise two pointers (one for
each array), call them `i` and `j`, to 0. If the elements at the indices `i` and
`j` are equivalent, save that element (or print it out directly) and advance
both pointers by one. If they are not equal, only advance the pointer of the
array that stores the lower value. Break out of this loop once one of the
pointers as marched through the entire array.

```python
def alg(a, b):
  N, NB = len(a), len(b)
  assert N == NB
  i, j = 0, 0
  ans = []
  while i<N and j<N:
    ca, cb = a[i], b[j]
    if ca < cb:
      i += 1
    elif ca > cb:
      j += 1
    else:
      ans.append(ca)
      i += 1
      j += 1

  return ans
```

### 1.4.5 Tilde Approximations

---

e.
$\frac{lg(2N)}{lg(N)} = \frac{lg(2)+lg(N)}{lg(N)} = \frac{1+lg(N)}{lg(N)}
\sim 1$

f. $\frac{lg(N^2 +1)}{lg(N)} = \frac{2\ lg(N)}{lg(N)} \sim 2$

g. $\frac{N^100}{2^N} \sim \frac{N^100}{2^N}$

_Note, that g. goes to $0$ for $N$ going to infinity, since the denominator is
growing faster than the numerator, and thus, by definition we cannot find a
tilde-approximations except for the expression itself, since denominators cannot
be 0._

### 1.4.1 Triplets from N Items

---

We are to prove that the number of triplets that can be chosen from $N$ elements
is precisely $N(N-1)(N-2)/6$ using mathematical induction.

We start with the base-case, which is checking that the statement holds for the
lowest possible $N$, in which we can find triplets, which is 3. Here we would
expect the closed formula to return 1, since there is only combination of
triplets:

$$

3(3-1)(3-2)/6 = 6/6 = 1


$$

We assume the statement to be generally true for any $N$ (inductive hypothesis).
We can rewrite the statement:

$$

P(N) = N(N-1)(N-2) / 6


$$

Now, we need to prove that P(N+1) is true, assuming our inductive hypothesis to
be true. By adding a single element to the sequence, we create `N choose 2` new
triplets. We can therefore write our inductive step as:

$$
\begin{align*}
P(N+1) &= P(N) + N! / 2!(N-2)! \\ &= P(N) + N(N-1) / 2 \\ &= N(N-1)(N-2)/6 +
N(N-1) / 2 \\ &= N(N-1)(N-2)/6 + 3N(N-1) / 6 \\ &= N(N-1)(N-2) + 3N(N-1) / 6 \\
&= N(N-1)(N-2+3) / 6 \\ &= N(N-1)(N+1) / 6
\end{align*}
$$

### 1.4.24 Eggs off a building

---

Do a binary search. If the egg is not broken at the current floor, F is a on a
higher floor. If the egg is broken, we can reuse the solution from 1.4.10 to
find the lowest floor level where the egg breaks.

We now start at the first floor and double the floor number until the egg
breaks. This takes at most $lg\ F$ times. Then we do a binary search from the
last floor the egg didn’t break, up to the floor it did break. Which takes
$lg\ F$ for a combined time of $2lg\ F$. Notice that the last floor the egg
didn’t break at is halfway up to the level it broke (as we are doubling the
floor number) and we can therefore not search a larger part of the array than F
in the binary search.

### 1.4.25 Revisit Eggs off a building

---

We split the total number of floors $N$ in $\sqrt{N}$ equal parts and start our
egg throw at $\sqrt{N}$ height. If the egg does not break, we move a level up to
$2\sqrt{N}$. We continue to do so, until the first egg eventually breaks at
$(i)\sqrt{N}$. We now know, that the target floor $F$ must be between
$(i-1)\sqrt{N}$ and $(i)\sqrt{N}$. Thus, we can do a linear search, which takes
$\sqrt{N}$ in the worst case. Both operations: First finding the chunk of size
$\sqrt{N}$ in which the breaking floor must be, and finding the floor within
those take $\sqrt{N}$ time for a total time of $\sim 2\sqrt{N}$.

### 1.4.18 Local Minimum

---

We first check if there exists a local minimum in the middle of the array. If
yes, we return that index to successfully be a local minimum. If not, we check
the element to the left of the middle. If it is smaller than the middle element,
that that means, that there must exist a local minimum in the left half (because
of the edge case on the left side). Otherwise, there must be a local minimum on
the right side of the array. Thus, we can binary search through the array in the
right direction, depending on our check of the element to the left of the
current middle, resulting in a logarithmic time algorithm.

```python
def localmin(arr, l, h):
  if l>h:
    return None
  m = (l+h)//2

  if arr[m-1] > arr[m] and arr[m+1] > arr[m]:
    return m
  else:
    if arr[m-1] < arr[m]:
      return localmin(arr, l, m-1)
    else:
      return localmin(arr, m+1, h)
```

### 1.4.29 Steque with Two Stacks

---

We maintain two stacks, one for the head, one for the tail of the steque. We can
`push()` and `enqueue()` by simply pushing on the corresponding stacks (constant
time operations). On a call to `pop()`, there are two cases. If the `headstack`
is nonempty, then we can simply call `pop()`, which we know to be constant. If
the `headstack` is empty, we have to `push` all elements of the tailstack in
reverse order onto the headstack, so that we can access the most least recently
enqueue object (the next to pop) on the top of the new headstack. This
operations takes linear time. But since, it occurs rarely, `pop()` runs in
amortised constant time.

```python
class Steque():
  def __init__(self):
    self.tailstack = Stack()
    self.headstack = Stack()

  def push(self, x):
    self.headstack.push(x)

  def enqueue(self, x):
    self.tailstack.push(x)

  def pop(self):
    if self.headstack.is_empty():
      while not self.tailstack.is_empty():
        self.headstack.push(self.tailstack.pop())
    return self.headstack.pop()
```

### 1.4.30 Deque with a Stack and Steque

---

```python
class Deque():
  def __init__(self):
    self.stack = Stack()
    self.steque = Steque()

  def pushLeft(self, x):
    self.stack.push(x)

  def pushRight(self, x):
    self.steque.push(x)

  def popLeft(self):
    if self.stack.is_empty():
      for i in range(self.steque.n):
        if i < n//2:
          self.steque.enqueue(self.steque.pop())
        else:
          self.stack.push(self.steque.pop())
    return self.stack.pop()

  def popRight(self):
    if self.steque.is_empty():
      for i in range(self.stack.n):
        self.steque.push(self.stack.pop())
    return self.steque.pop()
```

### 1.4.31 Deque with Three Stacks

---

```python
class Deque():
  def __init__(self):
    self.leftStack = Stack()
    self.rightStack = Stack()
    self.tempStack = Stack()

  def pushLeft(self, x):
    self.leftStack.push(x)

  def pushRight(self, x):
    self.rightStack.push(x)

  def popLeft(self):
    if self.leftStack.is_empty():
      N = self.rightStack.n
      for i in range(N):
        if i < N//2:
          self.tempStack.push(self.rightStack.pop())
        else:
          self.leftStack.push(self.rightStack.pop())

    return self.leftStack.pop()

  def popRight(self):
    if self.rightStack.is_empty():
      N = self.leftStack.n
      for i in range(N):
        if i < N//2:
          self.tempStack.push(self.leftStack.pop())
        else:
          self.rightStack.push(self.leftStack.pop())

    return self.rightStack.pop()
```
