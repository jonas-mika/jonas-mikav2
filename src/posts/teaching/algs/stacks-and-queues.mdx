---
id: stacks-and-queues
title: Stacks and Queues
tags: []
published: 02-07-2022
lastEdited: 02-05-2023
---

In this first chapter on data structures, we will have a look at the most basic,
but likewise most fundamental types of data structures: **Linear Data
Structures.**

> A data structure where data elements are arranged sequentially or linearly
> where the elements are attached to its previous and next adjacent in what is
> called a linear data structure. We can traverse (=durchqueren/ iterate) such
> data structures in a single run.

Opposed to the linear data structures that we will deal with in this chapter,
there are also non-linear data structures (one example are graphs). Non-linear
data structures are not sequentially arranged and can therefore not be traversed
in a single run.

## Linked Lists

---

Linked Lists are among the simplest and most common data structures. They are
ie. used to implement several other common abstract data types, including lists,
stacks, queues, associative arrays, though it is not uncommon to implement those
data structures without using a linked list as the basis.

> A **linked list** is a linear collection of data elements whose order is not
> given by their physical placement in memory (_as in arrays_). Instead, each
> element holds a value and a pointer to the next element.

### Advantages/ Disadvantages

---

- Allows efficient data insertion and removal without reallocation or
  reorganisation of the entire data structure (simply changing one pointer)

- Dynamic Data Structure (= The length of the sequence can increase and decrease
  efficiently)

- More memory consuming than arrays, since linked list requires to store the
  element and the pointer to the next node for each element.

- Doesn't allow random access to data (we can not index the sequence as
  efficient as with arrays) since the nodes in a linked list must be read in
  order from the beginning.

## Arrays

---

Arrays are among the _oldest and most important data structures_, and are used
by almost every program. They are also used to implement many other data
structures, such as _lists_ and _strings_. They effectively exploit the
addressing logic of computers. In most modern computers and many external
storage devices, the memory is a one-dimensional array of words, whose indices
are their addresses. Processors, especially vector processors, are often
optimised for array operations.

Arrays are also used to implement other data structures, such as lists, stacks,
queues, heaps, hash tables, dequeue, strings, and more.

> An **array** is a data structure consisting of a collection of elements
> (values or variables), each identified by an array index. An array is stored
> such that the position of each element can be computed from its index tuple by
> a mathematical formula. The simplest type of data structure is a linear array,
> also called one-dimensional array.

### Advantages/ Disadvantages

---

- Fast random access to data, through indexing (which is referencing to unique
  memory address, which always takes the same time, thus O(1) (constant time
  complexity) for accessing all elements inside the array regardless of the size
  of the array)

- More efficient in memory usage compared to linked list, since pointers do not
  need to be stored.

- High Time Complexity if we insert or remove element at position 0, since every
  index needs to change.

- Difficulties in implementing arrays to dynamically fit its contents.

### Static/ Dynamic Arrays

---

Because of the static nature of storage devices, arrays are usually statically
defined, meaning that prior to populating an array with information, we have to
give it a fixed size. Trying to insert more elements than initially declared, is
not possible.

Thus, by default, especially in more traditional programming languages, such as
`C`, `C++` or `Java` arrays are defined with a fixed size (unlike python's
dynamic array `list` type). However, we find ourselves often in the situation,
were we would like our container of information to increase in capacity over
time, since it can be impossible to foresee, how filled the array is going to
be.

We thus, have to think about how to increase the size of the array dynamically.
When trying to implement the dynamic array data structure, the initial challenge
that arises is, of when to adjust the size of the array, such that we neither
get a stack overflow error (ie. more elements than the array can store) nor we
waste a lot of memory (ie. we initialise a huge array to prevent stack overflow
errors). Both the overflow- and underflow defects can be addressed using dynamic
arrays, meaning arrays that increase in size if specific conditions at push and
pop occur.

1. **Increasing/ Decreasing by 1**

   The initial thought is to increase and decrease the array dynamically
   (dynamic array) by one. This however requires to copy all old elements into a
   new array of len(N)+1 and then insert the new one. Since this requires
   quadratic time $O(n²)$ for $n$ pushes it is highly inefficient and infeasible
   in real-world.

2. **Doubling/ Halving Array**

   It is a better idea to double the size of the array it is full and we halve
   the size of the array when the array is one-quarter full on removal of an
   element. This reduces the time complexity from one quadratic to amortised
   constant time complexity, since the many low-cost insertions pay for the
   occasionally occurring high-cost resizing operations. Such a data structure
   is able to compete with the constant time complexity for insertion and
   removal of elements of linked lists.

## Bags

---

A bag is a collection where removing items is not supported — its purpose is to
provide clients with the ability to collect items and then to iterate through
the collected items. As defined in the below API, the client should also be able
to query the bag if it is empty and find the number of items stored in the bag.
Thinking of bags, as literal bags that store some generic item (ie. marbles) is
helpful. We do not care about the order of the elements in the bag, nor can we
retrieve a specific one (like the most or least recently added).

For the source code of a bag (both implemented with a linked-list and dynamic
array data structure), see either:

1. [My GitHub](https://github.com/mikasenghaas/data-structures-and-algorithms/blob/master/fundamentals/bag.py)
2. [itu.algs4.fundamentals.bag](https://github.com/itu-algorithms/itu.algs4/blob/master/itu/algs4/fundamentals/bag.py)

## Stacks (=LIFO)

---

In computer science, a **stack** (or: _pushdown stack)_ is an abstract data type
that serves as a collection of elements, with two main principal operations:

**Push**: Adds an element to the collection **Pop**: Removes the most recently
added element

The order in which elements come off a stack gives rise to its alternative
name,  **LIFO** (**last in, first out**). Note, that in stacks the `push` and
`pop` operations occur only at one end of the structure, referred to as the top
of the stack. Additionally, a peek operation may give access to the top without
modifying the stack. The name "stack" for this type of structure comes from the
analogy to a set of physical items stacked on top of each other. This structure
makes it easy to take an item off the top of the stack, while getting to an item
deeper in the stack may require taking off multiple other items first.

For the source code of a stack (both implemented with a linked-list and dynamic
array data structure), see either:

1. [My GitHub](https://github.com/mikasenghaas/data-structures-and-algorithms/blob/master/fundamentals/stack.py)
2. [itu.algs4.fundamentals.stack](https://github.com/itu-algorithms/itu.algs4/blob/master/itu/algs4/fundamentals/stack.py)

## Queues (=FIFO)

---

In computer science, a **queue** is an abstract data type that serves as a
collection of elements, with two main principal operations:

**Enqueue**: Adds an element to the collection **Dequeue**: Removes the least
recently added element

The order in which elements are inserted to the rear (_Rückseite_) of the queue
and removed from the front of the queue, gives rise to the fact that queue are
based on the **first-in-first-out (FIFO) policy**. This policy of doing tasks in
the same order that they arrive is one that we encounter frequently in everyday
life: from people waiting in line at a theatre, to cars waiting in line at a
toll booth, to tasks waiting to be serviced by an application on your computer.
In addition to the two characteristic methods, the data type should also be able
to be queried for emptiness, the least recently added element (next
`.dequeue()`), and the number of elements stored in the data type.

For the source code of a queue (both implemented with a linked-list and dynamic
array data structure), see either:

1. [My GitHub](https://github.com/mikasenghaas/data-structures-and-algorithms/blob/master/fundamentals/queue.py)
2. [itu.algs4.fundamentals.queue](https://github.com/itu-algorithms/itu.algs4/blob/master/itu/algs4/fundamentals/queue.py)

## Exercise Solutions

### 1.2.6: Circular Strings

---

A string `s` is circular to a string `t`, if and only if a circular shift of the
letters in one of the strings can reproduce the other string. For this to be
true, the two strings must have equivalent lengths and we can design a quadratic
time algorithm like so to check for the condition for any two strings:

```python
def is_circular(s, t):
  assert len(s) == len(t), "strings cannot be circular if lengths don't match"
  n = len(s)

  for i in range(n):
    for j in range(n):
      if s[j] != t[(i+j)%n]:
        break
      if j == n-1:
        return True
  return False
```

An algorithmically faster solution is the following: We can simply check,
whether the substring `t` is contained in the string `s` concatenated to itself.
This search uses the `Boyer-Moore` (or similar) string-search algorithm, which
takes $O(N)$ on average and $O(NM)$ in the worst case, where $N$ is the length
of the larger string (search string) and $M$ is the length of the matching
pattern. Since for $s$ and $t$, both having length $n$, the average case is
$\sim 2n$, which is $O(n)$ on average, but still quadratic in the worst-case.

```python
def is_cicrular(s, t):
  return t in s+s
```

### 1.3.7: Peek-Method

---

The method `peek()` to a stack should return the most recently added element. In
a single-linked list implementation of a `Stack`, we maintain a sequence of
linked `Node` objects that each store an `item` and a pointer to the `next`
node. The stack-object usually has a reference to the first (top) item of the
stack, so that we can easily facilitate `pop` and `push` operations on the
stack. Given this knowledge, we can design the `peek`-method rather simply,
using the same notation as the `itu.algs4` library in _constant time_ ($O(1)$):

```python
def peek(self):
  if self.is_empty():
    raise ValueError("Stack underflow")

  return self._first.item
```

### 1.3.19: RemoveLast-Method

---

To implement the `remove_last` method, the entire sequence of node objects needs
to be traversed, until the last node, which does not have any further pointer to
a node (`null`), is reached. Once this node is reached, setting the pointer of
the previous node, so the second-last node to `null`, sovles the problem. The
garbage collector of the programming language used, should detect, that the old
last node is not part of the data structure anymore, since it is not pointed to
from anywhere.

```python
def remove_last(self):
  curr = self._first
  while curr.next.next != None:
    curr = curr.next
  curr.next = None
  self._n -= 1
```

### 1.3.20: Delete-Method

---

To implement the `delete(k)`-method, a similar algorithm as the one for
`remove_last` can be used. If the $k=1$ means, that the most recently added
element needs to be removed, then we iterate to the $k-1$-th element and set
that pointer to the $k+1$ element. This looks like this in code.

```python
def delete(self, k):
  # assumes elements to be indexed from [1..N]
  # (from most recent to least recent)
  assert k>0 and k <= self._n, 'delete elements in the range [1, n]'

  if k==1:
    self.pop()
    return

  curr = self._first
  for i in range(k-2):
    curr = curr.next
  curr.next = curr.next.next
  self._n -= 1
```

### 1.3.21: Find-Method

---

To implement `find` on a linked-list, we simply iterate over the entire linked
list. Once, the currently looked at node contains the element, we are looking
for, we return `True`. If the entire linked list was traversed, without finding
the element, we return `False`.

```python
def find(self, el):
  curr = self._first
  while curr != None:
    if curr.item == el:
      return True
    curr = curr.next
  return False
```

### 1.3.22: Linked-List Operation

---

The following operation on a linked list node `x` for a new node `t`

```python
t.next = x.next;
x.next = t;
```

inserts the node `t` directly after node `x`.

### 1.3.23: Wrong Linked-List Operation

---

The below operation does not serve the same purpose as the one from `1.3.22`,
since, since with the first operation `x.next` becomes updated to `t`, thus
`t.next` will point to itself after the second line is being executed. Thus the
linked list structure ends, and all nodes previously following `x` can now not
be reached.

```python
x.next = t;
t.next = x.next;
```

### 1.3.26: Max-Method

---

Finding the `max` item stored in a linked-list requires to iterate over the
entire sequence of linked-list nodes ($O(N)$) and dynamically maintaining a
reference to the element that is so-far the maximum element.

```python
def max(self):
  curr = self._first
  m = curr.item
  while curr.next != None:
    curr = curr.next
    if curr.item > m:
      m = curr.item
  return m
```

### 1.3.28: Recursive Max-Method

---

We can write the `max` method (just like any of the other operation so far
looked at for the linked list in a recursive fashion, like so:

```python
def recursive_max(self):
  return self._max(self._first, 0)

def _max(self, curr, m):
  if curr == None:
    return m
  if curr.item > m:
    m = curr.item
  return self._max(curr.next, m)
```

### 1.3.24: RemoveAfter-Method

---

Given a node, we can simply put the pointer of the node to the node two nodes
further, in order to remove the node immediately following the node given as
input. The operation is only done, if the node is not the last node in the
linked-list (check `node.next != None`)

```python
def remove_after(node):
  if node and node.next != None:
    node.next = node.next.next
```

### 1.3.25: InsertAfter-Method

---

Given two linked list nodes `n1` and `n2`, the tasks is to insert `n2` directly
after `n1` and we can assume `n1` to be part of a linked-list. We can perform
the operation by simply putting the pointer of `n2` to the node following `n1`
and update `n1` pointer to `n2` (just like in`1.3.22`).

```python
def insert_after(n1, n2):
  if n1!=None and n2!=None:
    n2.next = n1.next
    n1.next = n2
```

### 1.3.31: Doubly-Linked Lists

---

The primary difference is the additional pointer to the previous node in the
list, which needs to be considered for all the operations.

### 1.3.48: Deque

---

We consider the left side of the deque to be `stack1` and the right side of the
deque to be `stack2`. So if we push something to stack1, we call `pushLeft()` to
the deque, and if we pop something from stack1 we call `popLeft()`. And likewise
for stack2 with `pushRight()` and `popRight()`. To avoid an empty stack drawing
from the bottom of the other stack, we keep track of the current number of
elements in both stacks and only allow pop() if there are elements remaining in
that particular stack.
