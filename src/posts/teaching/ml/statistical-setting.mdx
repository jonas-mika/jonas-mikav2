---
id: statistical-setting
title: Statistical Setting of Machine Learning
tags: []
published: 08-30-2022
lastEdited: 08-30-2022
---

Machine Learning, though sounding fancy at times, is nothing more than inferring information about data relationships statistically. So, with the
knowledge of $Y \sim \bold{X}$, the relationship between a set of features $\bold{X}$ and some response $Y$, we can make a prediction about the value
of $Y$ if we have collected data on the set of features. This is the general setting in most machine learning problems (regression, classification,
â€¦):

1. Collect some data about a _set of features $\bold{X}$_ (also called _predictors_/ _dependent variables_) and a response $Y$ (_independent
   variable_)
2. Learn the relationship between $\bold{X}\sim Y$ from the data set (training split) by inferring a decision function $\hat{f}$
3. Use the decision function to make a prediction $\hat{y}=\hat{f}(\bold{X})$ about unseen data or to infer knowledge about the relationship of
   $\bold{X}\sim Y$.

Letâ€™s dive deeper into the statistical setting of machine learning experiments.

### **Modelling Relationships of Random Variables**

---

We want to model some target variable $Y$ (whether it be quantitative or qualitative) by using a set of $p$ features $X_1, X_2, ...,X_p$. This
mathematical relationship can be easily modelled through some function $f$. There is some error term $\epsilon$ involved in this relationship, ie.
when the selected features do not fully describe the distribution of $Y$ and also because $Y$ itself might involve some randomness.

$$
Y=f(X)+\epsilon
$$

In essence, machine learning evolves around a set of approaches for estimating $f$ in a way that we make a maximally sound decision in the statistical
sense.

### **Using the Decision Function**

---

There are two main reasons why to estimate $f$, the function that models the relationship between the set of features and the response.

The most straightforward reason and also the basic idea of machine learning is, that we can use the function to make **predictions** of the target
variable by just feeding a set of measured features. Mathematically, we denote:

$$
\hat{Y}=\hat{f}(X)
$$

_Note, we can vectorise the decision function to not only take a single data point, but a whole matrix $\bold{X}$ of $n$ observed samples. The
function then outputs the vector of predictions $\bold{y}$._

Generally, the accuracy of our prediction $\hat{Y}$ depends on some _reducible error_ and the _irreducible error_. The function $\hat{f}$ will never
be the function that actually models the distribution of the response $Y$, since it is more than unlikely that our set of features $X$ fully describes
the response variable. Thus, there exists no such function $\hat{f}$ in the input of $X$ that is always right. Since for some fixed $X$ this
uncertainty cannot be reduced by adjusting the function $f$, this is usually referred to as the _irreducible error._ If our measured features are just
noise, so don't contain any information about the behaviour of $Y$, then the most sophisticated methods of estimating $f$ will still yield a bad
performance: this is commonly referred to as garbage-in-garbage-out.

<aside>
ðŸ’¡ Our measured features don't perfectly describe the response $Y$, and thus there is no way of finding a 100%-accuracy function $f$. This is referred to as the irreducible error.

</aside>

However, there is some _reducible error,_ that is determined by our choice of $f$. Out of the space of all functions $f:X\rightarrow Y$, there are
some that are less and some that are more accurately describing the true relationship between the set of features and the response. It is the task of
statisticians to find the best-possible estimate for $f$. Different ML models in different settings are concerned with finding the function $f$ that
minimises the _reducible error._

We must not necessarily use our estimate $\hat{f}$ that models the relationship between some features $X$ towards the target variable $Y$ to make
predictions $\hat{Y}$ through $\hat{f}(X)$. We can also use the function to infer knowledge about the relationship between the features and response.
We refer to these types of usecases as **statistical inference**. Common questions are:

1. Which features are associated with the response?
2. What is the relationship between the response and each feature?
3. Can the relationship be adequately summarised using a linear equation, or is the relationship more complicated?

### **Estimating the Decision Function**

---

The field of machine learning evolves around finding estimates for $f$ that we can use to make predictions. We will discuss multiple linear and
non-linear approaches (so we either assume the relationship to be linear, ie. $f$ is a linear function, or the relationship to be non-linear, ie. $f$
is a non-linear function).

Broadly speaking, most statistical learning methods for this task can be characterised as either _parametric_ or _non-parametric_.

**Parametric Methods**

---

Parametric methods involve a two-step model-based approach.

1. First, we make an assumption about the functional form, or shape, of $f$. Even though we don't know the exact form of the function that best
   describes the relationship between a set of features $X$ and some target variable $Y$, we assume it to be of some form. The simplest way is to
   assume $f$ to be linear in $X$, st.:

   $$
   f(X)=\beta_0+\beta_1X_1+\beta_2X_2+...+\beta_p X_p
   $$

   Choosing $f$ in that way is known as the linear model. Assuming some underlying model greatly simplifies the problem of estimating $f$, since we
   have reduced the problem to simply finding estimates for the model parameters, here $\beta_0, \beta_1, ...\beta_p$. Instead of having to estimate
   an arbitrary $p$-dimensional function $f(X)$, one now only needs to estimate $p+1$ coefficients.

2. Now, we only need a procedure that uses training data to _fit_ or _train_ the model, that is finding estimates for the model parameters, sometimes
   called *weight*s. Usually, we define some loss function and then use an algorithmic procedure known as _gradient descent_ to iteratively update our
   model parameters to minimise the loss.

**Non-Parametric Methods**

---

In contrast to parametric methods, non-parametric methods do not make any explicit assumptions about the form of $f$ in advance.

This completely avoids the major disadvantage of parametric methods, that the initially chosen functional form of $\hat{f}$ might be inadequate to
describe the true relationship between the features and response variable, and thus, we will never get a good approximate for $f$, since we do not
make any initial assumptions for $f$.

Non-parametric methods are much more flexible, but this comes with two major downsides:

1. Since we do not assume a functional form for $f$, we have not reduced the problem to finding estimates for the function parameters. Because of
   this, a very large number of observations and a lot more computing power (and thus training time) is required in order to obtain an accurate
   estimate for $f$. That is the reason, why more complex models like neural networks, only give reasonable results for large training datasets and
   typically take a longer time to train.
2. If we do not assume any prior form for $f$, but allow full flexibility, it is likely that $f$ will take on a such complex form, that prevents us
   from being able to infer something about the relationship between $X$ and $Y$. If we solely want to use $\hat{f}$ to make predictions about $Y$
   based on $\hat{f}(X)$, then this might be fine, but if our goal is statistical inference then non-parametric methods are usually a bad choice.

This approach for estimating $f$ is known as parametric since it reduces the problem of estimating $f$down to one of estimating a set of parameters.
While this greatly simplifies the problem, it comes with the obvious downside, that the model we choose usually does not match the true unknown form
of $f$, and thus, there is a pre-set limit of how good we can describe $Y$ using $f(X)$. Choosing a less-restrictive non-parametric approach to
estimate $f$ avoids the above-mentioned problem, but comes at the price of large training datasets and training time and poorer interpretability of
the final estimate of $f$. This is often referred to as the trade-off between prediction accuracy and model interpretability.

### **Evaluating the decision function - Measuring the Quality of Fit**

---

It is difficult to say, how good of a fit our estimate for $f$ (the true relationship between the predictors $\bold{X}$ and the target vector $Y$ is)
without quantifying it. Sure, if the number of features $p$ is at most $2$, then we are able to visualise it and get an intuition of how good of a fit
our estimated $\hat{f}$ is given the data. However, this is mathematically imprecise and also gets conceptually difficult for the generalisation of
$p>2$ features, since we cannot visualise more than three-dimensional space.

We, therefore, need to be able to assess the performance of a model mathematically. So for some model, that predicts some quantity (whether it be
quantitative or qualitative), we would like to assign some numeric value that is high, if the predictions differ a lot from the true values and a
numeric value of $0$, if everything is predicted perfectly.

In machine learning, such a function is referred to as a **loss function**. It is a function $L(y, \hat{f}(x))$ that takes in the predicted values
from our estimated decision function $\hat{f}$ and the true values and returns a numeric value in the range $R_L=[\ 0, \infty \ )$ (usually, MBE
allows for negative loss). Note, that the loss-function can be written in many different ways:

$$
L(y, \hat{y})=L(y, \hat{f}(\bold{X}))=L(y, d(\bold{X}))
$$

_Generally, the loss function is a function that simply takes in two vectors of the same length and assigns some numeric value that measures the
quality of the predictions. However, if $\hat{f}$ is simple, ie. only has two parameters, like the simple linear regression model (predicting a
quantitative response using a single predictor), then we can use calculus to find the parameters that minimise the loss, and thus find the best fit
for the training data._

Whether we are in a classification or regression setting, influences the choice of loss function, and even for the same type of problem, the choice of
a loss function is completely open, although there are some typically used loss functions. These are listed in the section List of Loss-Functions.

**Training Loss vs. Test Loss**

When we evaluate the quality of fit using our defined loss function for our model estimate $\hat{f}$ on the training set, ie.
$L(y_\text{train}, d(\bold{X}_\text{train}))$, then we have computed the so-called **training loss**. While this is a property that is interesting to
look at (especially when dealing with low variance models, such as linear regression models), we are usually less interested in the training loss.
That is, because it is quite easy to make a model (our function $\hat{f}$) pick up all the information (and noise) from our data, by perfectly fitting
all points. In that case, we have found a perfect fit for the training data, but the model is likely to perform very poorly on unseen data of the same
type since it picked up a lot of noise.

So, the fact that $\hat{f}(x_i)\approx y_i$ for any point $(x_i,y_i)$ in our training set in itself is not really impressive and even not desirable
for flexible models. Instead, we want our model to generalise to unseen data, that is: _We want to maximise the accuracy (minimise the loss) of
predictions of never seen test data_. For some data point $(x_0, y_0)$ that was not part of the training set, we would like to achieve that generally
our model estimate $\hat{f}(x_0)\approx y_0$. It is therefore desirable to choose the model with the lowest **test loss**. Mathematically, this is the
average expected loss, which we can minimise as follows:

$$
min(E[L(y, d(\bold{X}))])
$$

We will explore methods to choose the model that generalises best later. Generally, one can say, that the more flexible (complex, more degrees of
freedom) a model is, the more it is prone to overfit (high performance on the training set, low performance on test set - doesn't generalise).
