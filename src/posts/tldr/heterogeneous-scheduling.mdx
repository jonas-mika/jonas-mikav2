---
title: Decentralised Training of Foundation Models in Heterogeneous Environments
organisation: ETH, Stanford, Meta AI
authors:
  - firstName: Binhang
    lastName: Yuan
  - firstName: Yongjun
    lastName: He
  - firstName: Jared Quincy
    lastName: Davis
links:
  - title: arXiv
    href: https://arxiv.org/abs/2206.01288
released: 2022-06-02
published: 2024-09-11
lastEdited: 2024-09-11
publish: true
---

<List>
    <ListItem title="Introduction">
    Introduces a novel scheduling algorithm for decentralised, parallelised training of foundation models, achieving up to 4.8x speedup over state-of-the-art approaches by optimising communication between heterogeneous compute resources
    </ListItem>
    <ListItem title="Motivation">
    Unlock untapped decentralised, heterogeneous compute resources for training huge-scale foundation models
    </ListItem>
    <ListItem title="Problem Statement">
    Optimises communication requirements by solving an allocation problem over a graph, minimising communication costs for data and pipeline parallelism
    </ListItem>
    <ListItem title="Methodology">
    Proposes a hybrid genetic algorithm to solve allocation problem
    </ListItem>
    <ListItem title="Results">
    Experimental results demonstrate up to 4.8x throughput gain in worldwide geo-distributed workers setup when compared to state-of-the-art methods like Megatron and DeepSpeed
    </ListItem>
</List>